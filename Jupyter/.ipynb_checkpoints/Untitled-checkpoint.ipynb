{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a3dfff8-9d60-418c-a5fd-a4adf7a121b1",
   "metadata": {},
   "source": [
    "IMP spatiotemporal tutorial {#mainpage}\n",
    "========\n",
    "\n",
    "# Introduction {#introduction}\n",
    "\n",
    "Biomolecules are constantly in motion; therefore, a complete depiction of their function must include their dynamics instead of just static structures. We have developed an integrative spatiotemporal approach to model dynamic systems.\n",
    "\n",
    "Our approach applies a composite workflow, consisting of three modeling problems to compute (i) heterogeneity models, (ii) snapshot models, and (iii) trajectory models.\n",
    "Heterogeneity models describe the possible biomolecular compositions of the system at each time point. Optionally, other auxiliary variables can be considered, such as the coarse location in the final state when modeling an assembly process.\n",
    "For each heterogeneity model, one snapshot model is produced. A snapshot model is a set of alternative standard static integrative structure models based on the information available for the corresponding time point.\n",
    "Then, trajectory models are created by connecting alternative snapshot models at adjacent time points. These trajectory models can be scored based on both the scores of static structures and the transitions between them, allowing for the creation of trajectories that are in agreement with the input information by construction.\n",
    "\n",
    "If you use this tutorial or its accompanying method, please site the corresponding publications:\n",
    "\n",
    "- Latham, A.P.; Tempkin, J.O.B.; Otsuka, S.; Zhang, W.; Ellenberg, J.; Sali, A. bioRxiv, 2024, https://doi.org/10.1101/2024.08.06.606842.\n",
    "- Latham, A.P.; Rožič, M.; Webb, B.M., Sali, A. in preparation. (tutorial)\n",
    "\n",
    "# Integrative spatiotemporal modeling workflow {#steps}\n",
    "\n",
    "In general, integrative modeling proceeds through three steps (i. gathering information; ii. choosing the model representation, scoring alternative models, and searching for good scoring models; and iii. assessing the models). In integrative spatiotemporal modeling, these three steps are repeated for each modeling problem in the composite workflow (i. modeling of heterogeneity, ii. modeling of snapshots, and iii. modeling of a trajectory).\n",
    "\n",
    "<img src=\"images/Overview.png\" width=\"300px\" />\n",
    "\n",
    "This tutorial will walk you through the creation of a spatiotemporal model for the  hypothetical assembly mechanism of the Bmi1/Ring1b-UbcH5c complex. We note that all experimental data besides the static structure used in this study is purely hypothetical, and, thus, the model should not be interpreted to be meaningful about the actual assembly mechanism of the complex.\n",
    "\n",
    "Finally, this notebook is intended to present an abbreviated version of this protocol, with the computationally expensive steps excluded. A more complete version of this tutorial can be found as a series of python scripts at https://github.com/salilab/imp_spatiotemporal_tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d93b4-3b97-4187-867c-977f8353f4aa",
   "metadata": {},
   "source": [
    "Modeling of heterogeneity {#heterogeneity}\n",
    "====================================\n",
    "\n",
    "Here, we describe the first modeling problem in our composite workflow, how to build models of heterogeneity modeling using IMP. In this tutorial, heterogeneity modeling only includes protein copy number; however, in general, other types of information, such as the coarse location in the final state, could also be included in heterogeneity models.\n",
    "\n",
    "# Heterogeneity modeling step 1: gathering of information {#heterogeneity1}\n",
    "\n",
    "We begin heterogeneity modeling with the first step of integrative modeling, gathering information. Heterogeneity modeling will rely on copy number information about the complex. In this case, we utilize the X-ray crystal structure of the fully assembled Bmi1/Ring1b-UbcH5c complex from the protein data bank (PDB), and synthetically generated protein copy numbers during the assembly process, which could be generated from experiments such as flourescence correlation spectroscopy (FCS).\n",
    "\n",
    "<img src=\"images/Input_heterogeneity.png\" width=\"600px\" />\n",
    "\n",
    "The PDB structure of the complex informs the final state of our model and constrains the maximum copy number for each protein, while the protein copy number data gives time-dependent information about the protein copy number in the assembling complex.\n",
    "\n",
    "# Heterogeneity modeling step 2: representation, scoring function, and search process {#heterogeneity2}\n",
    "\n",
    "Next, we represent, score and search for heterogeneity models models. A single heterogeneity model is a set of protein copy numbers, scored according to its fit to experimental copy number data at that time point. As ET and SAXS data, are only available at 0 minutes, 1 minute, and 2 minutes, we choose to create heterogeneity models at these three time points. We then use `prepare_protein_library`, to calculate the protein copy numbers for each snapshot model and to use the topology file of the full complex (`spatiotemporal_topology.txt`) to generate a topology file for each of these snapshot models. The choices made in this topology file are important for the representation, scoring function, and search process for snapshot models, and are discussed later. For heterogeneity modeling, we choose to model 3 protein copy numbers at each time point, and restrict the final time point to have the same protein copy numbers as the PDB structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fae319-6215-4712-835d-b09606974260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%colabonly\n",
    "# For colab, we need to install IMP\n",
    "!add-apt-repository -y ppa:salilab/ppa\n",
    "!apt install imp\n",
    "import sys, os, glob\n",
    "sys.path.append(os.path.dirname(glob.glob('/usr/lib/python*/dist-packages/IMP')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff4d3e5-04de-4092-8cfc-2ad3018675da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports for the tutorial\n",
    "import sys, os, glob\n",
    "from IMP.spatiotemporal import prepare_protein_library\n",
    "import shutil\n",
    "import IMP.spatiotemporal as spatiotemporal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b78a015-32e5-4701-8c6f-df14863ba9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for prepare_protein_library:\n",
    "times = [\"0min\", \"1min\", \"2min\"]\n",
    "exp_comp = {'A': '../../../modeling/Input_Information/gen_FCS/exp_compA.csv',\n",
    "            'B': '../../../modeling/Input_Information/gen_FCS/exp_compB.csv',\n",
    "            'C': '../../../modeling/Input_Information/gen_FCS/exp_compC.csv'}\n",
    "expected_subcomplexes = ['A', 'B', 'C']\n",
    "template_topology = 'spatiotemporal_topology.txt'\n",
    "template_dict = {'A': ['Ubi-E2-D3'], 'B': ['BMI-1'], 'C': ['E3-ubi-RING2']}\n",
    "nmodels = 3\n",
    "\n",
    "# calling prepare_protein_library\n",
    "prepare_protein_library.prepare_protein_library(times, exp_comp, expected_subcomplexes, nmodels,\n",
    "                                                template_topology=template_topology, template_dict=template_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56fbe48-da12-412e-ac2e-dca673e04a43",
   "metadata": {},
   "source": [
    "From the output of `prepare_protein_library`, we see that there are 3 heterogeneity models at each time point (it is possible to have more snapshot models than copy numbers if multiple copies of the protein exist in the complex). For each heterogeneity model, we see 2 files:\n",
    "- *.config, a file with a list of proteins represented in the heterogeneity model\n",
    "- *_topol.txt, a topology file for snapshot modeling corresponding to this heterogeneity model.\n",
    "\n",
    "# Heterogeneity modeling step 3: assessment {#heterogeneity_assess}\n",
    "\n",
    "Now, we have a variety of heterogeneity models. In general, there are four ways to assess a model: estimate the sampling precision, compare the model to data used to construct it, validate the model against data not used to construct it, and quantify the precision of the model. Here, we will focus specifically on comparing the model to experimental data, as other assessments will be performed later, when the trajectory models are assessed.\n",
    "\n",
    "Next, we must plot the modeled and experimental copy numbers simultaneously for each protein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fdd381-6af6-44fd-8f36-dbfc5d7e10b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: plotting script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf8ee82-4d59-4279-99dd-1d87b896fa15",
   "metadata": {},
   "source": [
    "From these plots, we observe that the range of possible experimental copy numbers are well sampled by the heterogeneity models, indicating that we are prepared for snapshot modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496f2687-bb05-4f51-8eab-a1170f6ac5fa",
   "metadata": {},
   "source": [
    "Modeling of snapshots {#snapshots}\n",
    "====================================\n",
    "\n",
    "Here, we describe the second modeling problem in our composite workflow, how to build models of static snapshot models using IMP. We note that this process is similar to previous tutorials of [actin](https://integrativemodeling.org/tutorials/actin/) and [RNA PolII](https://integrativemodeling.org/tutorials/rnapolii_stalk/).\n",
    "\n",
    "# Snapshot modeling step 1: gathering of information {#snapshots1}\n",
    "\n",
    "We begin snapshot modeling with the first step of integrative modeling, gathering information. Snapshot modeling utilizes structural information about the complex. In this case, we utilize heterogeneity models, the X-ray crystal structure of the fully assembled Bmi1/Ring1b-UbcH5c complex from the protein data bank (PDB), synthetically generated electron tomography (ET) density maps during the assembly process, and physical principles.\n",
    "\n",
    "<img src=\"images/Input_snapshot.png\" width=\"600px\" />\n",
    "\n",
    "The heterogeneity models inform protein copy numbers for the snapshot models. The PDB structure of the complex informs the structure of the individual proteins. The time-dependent ET data informs the size and shape of the assembling complex. physical principles inform connectivity and excluded volume.\n",
    "\n",
    "# Snapshot modeling step 2: representation, scoring function, and search process {#snapshots2}\n",
    "\n",
    "Next, we represent, score and search for snapshot models. This step is quite computationally expensive. Therefore, we will not run the modeling protocol in this notebook, though the scripts are available in `modeling/Snapshots/Snapshots_Modeling/`. Here, we will simply describe the important steps made by two scripts. The first, `static_snapshot.py`, uses IMP to represent, score, and search for a single static snapshot model. The second, `start_sim.py`, automates the creation of a snapshot model for each heterogeneity model.\n",
    "\n",
    "## Modeling one snapshot\n",
    "\n",
    "Here, we will describe the process of modeling a single snapshot model, as performed by running `static_snapshot.py`.\n",
    "\n",
    "### Representing the model {#snapshot_representation}\n",
    "\n",
    "We begin by representing the data and the model. In general, the *representation* of a system is defined by all the variables that need to be determined.\n",
    "\n",
    "For our model of a protein complex, we use a combination of two representations. The first is a series of *spherical beads*, which can correspond to portions of the biomolecules of interest, such as atoms or groups of atoms. The second is a series of *3D Gaussians*, which help calculate the overlap between our model and the density from ET data.\n",
    "\n",
    "Beads and Gaussians in our model belong to either a *rigid body* or *flexible string*. The positions of all beads and Gaussians in a single rigid body are constrained during sampling and do not move relative to each other. Meanwhile, flexible beads can move freely during sampling, but are restrained by sequence connectivity.\n",
    "\n",
    "To begin, we built a topology file with the representation for the model of the complete system, `spatiotemporal_topology.txt`, located in `Heterogeneity/Heterogeneity_Modeling/`. This complete topology was used as a template to build topologies of each heterogeneity model. Based on our observation of the structure of the complex, we chose to represent each protein with at least 2 separate rigid bodies, and left the first 28 residues of protein C as flexible beads. Rigid bodies were described with 1 bead for every residue, and 10 residues per Gaussian. Flexible beads were described with 1 bead for every residue and 1 residue per Gaussian. A more complete description of the options available in topology files is available in the the [TopologyReader](@ref IMP::pmi::topology::TopologyReader) documentation.\n",
    "\n",
    "```\n",
    "|molecule_name | color | fasta_fn | fasta_id | pdb_fn | chain | residue_range | pdb_offset | bead_size | em_residues_per_gaussian | rigid_body | super_rigid_body | chain_of_super_rigid_bodies | \n",
    "\n",
    "|Ubi-E2-D3|blue|3rpg.fasta.txt|Ubi-E2-D3|3rpg.pdb|A|-1,18|2|1|10|1|1||\n",
    "|Ubi-E2-D3|blue|3rpg.fasta.txt|Ubi-E2-D3|3rpg.pdb|A|19,147|2|1|10|2|1||\n",
    "|BMI-1|red|3rpg.fasta.txt|BMI-1|3rpg.pdb|B|3,83|-2|1|10|3|2||\n",
    "|BMI-1|red|3rpg.fasta.txt|BMI-1|3rpg.pdb|B|84,101|-2|1|10|4|2||\n",
    "|E3-ubi-RING2|green|3rpg.fasta.txt|E3-ubi-RING2|BEADS|C|16,44|-15|1|1|5|3||\n",
    "|E3-ubi-RING2|green|3rpg.fasta.txt|E3-ubi-RING2|3rpg.pdb|C|45,116|-15|1|10|6|3||\n",
    "```\n",
    "\n",
    "Next, we must prepare `static_snapshot.py` to read in this topology file. We begin by defining the input variables, `state` and `time`, which define which topology to use, as well as the paths to other pieces of input information.\n",
    "\n",
    "```python\n",
    "# Running parameters to access correct path of ET_data for EM restraint\n",
    "# and topology file for certain {state}_{time}_topol.txt\n",
    "state = sys.argv[1]\n",
    "time = sys.argv[2]\n",
    "\n",
    "# Topology file\n",
    "topology_file = f\"../{state}_{time}_topol.txt\"\n",
    "# Paths to input data for topology file\n",
    "pdb_dir = \"../../../../Input_Information/PDB\"\n",
    "fasta_dir = \"../../../../Input_Information/FASTA\"\n",
    "# Path where forward gmms are created with BuildSystem (based ont topology file)\n",
    "# If gmms exist, they will be used from this folder\n",
    "forward_gmm_dir = \"../forward_densities/\"\n",
    "# Path to experimental gmms\n",
    "exp_gmm_dir= '../../../../Input_Information/ET_data/add_noise'\n",
    "```\n",
    "\n",
    "Next, we build the system, using the topology tile, described above.\n",
    "```python\n",
    "# Create a system from a topology file. Resolution is set on 1.\n",
    "bs = IMP.pmi.macros.BuildSystem(mdl, resolutions= 1, name= f'Static_snapshots_{state}_{time}')\n",
    "bs.add_state(t)\n",
    "```\n",
    "\n",
    "Then, we prepare for later sampling steps by setting which Monte Carlo moves will be performed. Rotation (`rot`) and translation (`trans`) parameters are separately set for super rigid bodies (`srb`), rigid bodies (`rb`), and beads (`bead`).\n",
    "```python\n",
    "#  Macro execution: It gives hierarchy and degrees of freedom (dof).\n",
    "# In dof we define how much can each (super) rigid body translate and rotate between two adjacent Monte Carlo steps\n",
    "root_hier, dof = bs.execute_macro(max_rb_trans=1.0,\n",
    "                                  max_rb_rot=0.5, max_bead_trans=2.0,\n",
    "                                  max_srb_trans=1.0, max_srb_rot=0.5)\n",
    "```\n",
    "\n",
    "### Scoring the model {#snapshot_scoring}\n",
    "\n",
    "After building the model representation, we choose a scoring function to score the model based on input information. This scoring function is represented as a series of restraints that serve as priors.\n",
    "\n",
    "#### Connectivity\n",
    "\n",
    "We begin with a connectivity restraint, which restrains beads adjacent in sequence to be close in 3D space.\n",
    "\n",
    "```python\n",
    "# Adding Restraints\n",
    "# Empty list where the data from restraints should be collected\n",
    "output_objects=[]\n",
    "\n",
    "# Two common restraints: ConnectivityRestraint and ExcludedVolumeSphere\n",
    "# ConnectivityRestraint is added for each \"molecule\" separately\n",
    "for m in root_hier.get_children()[0].get_children():\n",
    "    cr = IMP.pmi.restraints.stereochemistry.ConnectivityRestraint(m)\n",
    "    cr.add_to_model()\n",
    "    output_objects.append(cr)\n",
    "```\n",
    "\n",
    "#### Excluded volume\n",
    "\n",
    "Next is an excluded volume restraint, which restrains beads to minimize their spatial overlap.\n",
    "\n",
    "```python\n",
    "# Add excluded volume\n",
    "evr = IMP.pmi.restraints.stereochemistry.ExcludedVolumeSphere(\n",
    "                                     included_objects=[root_hier],\n",
    "                                     resolution=1000)\n",
    "output_objects.append(evr)\n",
    "evr.add_to_model()\n",
    "```\n",
    "\n",
    "#### Electron tomography\n",
    "\n",
    "Finally, we restrain our models based on their fit to ET density maps. Both the experimental map and the forward protein density are represented as Gaussian mixture models (GMMs) to speed up scoring. The score is based on the log of the correlation coefficient between the experimental density and the forward protein density.\n",
    "\n",
    "```python\n",
    "# Applying time-dependent EM restraint. Point to correct gmm / mrc file at each time point\n",
    "# Path to corresponding .gmm file (and .mrc file)\n",
    "em_map = exp_gmm_dir + f\"/{time}_noisy.gmm\"\n",
    "\n",
    "# Create artificial densities from hierarchy\n",
    "densities = IMP.atom.Selection(root_hier,\n",
    "                 representation_type=IMP.atom.DENSITIES).get_selected_particles()\n",
    "\n",
    "# Create EM restraint based on these densities\n",
    "emr = IMP.pmi.restraints.em.GaussianEMRestraint(\n",
    "        densities,\n",
    "        target_fn=em_map,\n",
    "        slope=0.000001,\n",
    "        scale_target_to_mass=True,\n",
    "        weight=1000)\n",
    "output_objects.append(emr)\n",
    "emr.add_to_model()\n",
    "```\n",
    "\n",
    "### Searching for good scoring models {#snapshot_searching}\n",
    "\n",
    "After building a scoring function that scores alternative models based on their fit to the input information, we aim to search for good scoring models. For complicated systems, stochastic sampling techniques such as Monte Carlo (MC) sampling are often the most efficient way to compute good scoring models. Here, we generate a random initial configuration and then perform temperature replica exchange MC sampling with 16 temperatures from different initial configurations. By performing multiple runs of replica exchange MC from different initial configurations, we can later ensure that our sampling is sufficiently converged.\n",
    "\n",
    "```python\n",
    "# Generate random configuration\n",
    "IMP.pmi.tools.shuffle_configuration(root_hier,\n",
    "                                    max_translation=50)\n",
    "\n",
    "# Perform replica exchange sampling\n",
    "rex=IMP.pmi.macros.ReplicaExchange(mdl,\n",
    "        root_hier=root_hier,\n",
    "        monte_carlo_sample_objects=dof.get_movers(),\n",
    "        global_output_directory='output', # name 'output' is the best for imp sampcon select_good\n",
    "        output_objects=output_objects,\n",
    "        monte_carlo_steps=200, # Number of MC steps between writing frames.\n",
    "        number_of_best_scoring_models=0,\n",
    "        number_of_frames=500) # number of frames to be saved\n",
    "# In our case, for each snapshot we generated 25000 frames altogether (50*500)\n",
    "rex.execute_macro()\n",
    "```\n",
    "\n",
    "After performing sampling, a variety of outputs will be created. These outputs include `.rmf` files, which contain multi-resolution models output by IMP, and `.out` files which contains a variety of information about the run such as the value of the restraints and the MC acceptance rate.\n",
    "\n",
    "## Generalizing modeling to all snapshots {#snapshot_combine}\n",
    "\n",
    "Next, we will describe the process of computing multiple static snapshot models, as performed by running `start_sim.py`.\n",
    "\n",
    "From heterogeneity modeling, we see that there are 3 heterogeneity models at each time point (it is possible to have more snapshot models than copy numbers if multiple copies of the protein exist in the complex), each of which has a corresponding topology file in `Heterogeneity/Heterogeneity_Modeling/`. We wrote a function, `generate_all_snapshots`, which creates a directory for each snapshot model, copies the python script and topology file into that directory, and submits a job script to run sampling. The job script will likely need to be customized for the user's computer or cluster.\n",
    "\n",
    "```python\n",
    "# 1a - parameters for generate_all_snapshots\n",
    "# state_dict - universal parameter\n",
    "state_dict = {'0min': 3, '1min': 3, '2min': 1}\n",
    "\n",
    "main_dir = os.getcwd()\n",
    "topol_dir = os.path.join(os.getcwd(), '../../Heterogeneity/Heterogeneity_Modeling')\n",
    "items_to_copy = ['static_snapshot.py']  # additionally we need to copy only specific topology file\n",
    "# jobs script will likely depend on the user's cluster / configuration\n",
    "job_template = (\"#!/bin/bash\\n#$ -S /bin/bash\\n#$ -cwd\\n#$ -r n\\n#$ -j y\\n#$ -N Tutorial\\n#$ -pe smp 16\\n\"\n",
    "                \"#$ -l h_rt=48:00:00\\n\\nmodule load Sali\\nmodule load imp\\nmodule load mpi/openmpi-x86_64\\n\\n\"\n",
    "                \"mpirun -np $NSLOTS python3 static_snapshot.py {state} {time}\")\n",
    "number_of_runs = 50\n",
    "\n",
    "# 1b - calling generate_all_snapshots\n",
    "generate_all_snapshots(state_dict, main_dir, topol_dir, items_to_copy, job_template, number_of_runs)\n",
    "\n",
    "```\n",
    "\n",
    "# Snapshot modeling step 3: assessment {#snapshot_assess}\n",
    "\n",
    "The above code would variety of alternative snapshot models. In general, we would like to assess these models in at least 4 ways: estimate the sampling precision, compare the model to data used to construct it, validate the model against data not used to construct it, and quantify the precision of the model. In this portion of the tutorial, we focus specifically on estimating the sampling precision of the model, while quantitative comparisons between the model and experimental data will be reserved for the final step, when we assess trajectories. Again, this assessment process is quite computationally intensive, so, instead of running the script explicitly, we will walk you through the `snapshot_assessment.py` script, which is located in the `modeling/Snapshots/Snapshots_Assessment` folder.\n",
    "\n",
    "## Filtering good scoring models {#snapshot_filter}\n",
    "\n",
    "Initially, we want to filter the various alternative structural models to only select those that meet certain parameter thresholds. In this case, we filter the structural models comprising each snapshot model by the median cross correlation with EM data. We note that this filtering criteria is subjective, and developing a Bayesian method to objectively weigh different restraints for filtering remains an interesting future development in integrative modeling.\n",
    "\n",
    "The current filtering procedure involves three steps. In the first step, we look through the `stat.*.out` files to write out the cross correlation with EM data for each model, which, in this case, is labeled column `3`, `GaussianEMRestraint_None_CCC`. In other applications, the column that corresponds to each type of experimental data may change, depending on the scoring terms for each model. For each snapshot model, a new file is written with this data (`{state}_{time}_stat.txt`).\n",
    "\n",
    "```python\n",
    "# state_dict - universal parameter\n",
    "state_dict = {'0min': 3, '1min': 3, '2min': 1}\n",
    "# current directory\n",
    "main_dir = os.getcwd()\n",
    "\n",
    "# 1 calling extracting_stat_files function and related parameters\n",
    "keys_to_extract = [3]\n",
    "runs_nr = 50\n",
    "replica_nr = 16\n",
    "replica_output_name = 'output'\n",
    "decimals_nr = 16\n",
    "\n",
    "extracting_stat_files(state_dict, runs_nr, replica_nr, replica_output_name, keys_to_extract, decimals_nr)\n",
    "print(\"extracting_stat_files is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "```\n",
    "\n",
    "In the second step, we want to determine the median value of EM cross correlation for each snapshot model. We wrote `general_rule_calculation` to look through the `general_rule_column` for each `{state}_{time}_stat.txt` file and determine both the median value and the number of structures generated.\n",
    "\n",
    "```python\n",
    "# 2 calling general_rule_calculation and related parameters\n",
    "general_rule_column = '3'\n",
    "\n",
    "general_rule_calculation(state_dict, general_rule_column)\n",
    "\n",
    "print(\"general_rule_calculation is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "```\n",
    "\n",
    "In the third step, we use the `imp_sampcon select_good` tool to filter each snapshot model, according to the median value determined in the previous step. For each snapshot model, this function produces a file, `good_scoring_models/model_ids_scores.txt`, which contains the run, replicaID, scores, and sampleID for each model that passes filtering. It also saves RMF files with each model from two independent groups of sampling runs from each snapshot model to `good_scoring_models/sample_A` and `good_scoring_models/sample_B`, writes the scores for the two independent groups of sampling runs to `good_scoring_models/scoresA.txt` and `good_scoring_models/scoresB.txt`, and writes `good_scoring_models/model_sample_ids.txt` to connect each model to its division of sampling run. More information on `imp_sampcon` is available in the analysis portion of the [actin tutorial](https://integrativemodeling.org/tutorials/actin/analysis.html).\n",
    "\n",
    "```python\n",
    "# 3 calling general_rule_filter_independent_samples\n",
    "general_rule_filter_independent_samples(state_dict, main_dir)\n",
    "print(\"general_rule_filter_independent_samples is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "```\n",
    "\n",
    "## Plotting data, clustering models, and determining sampling precision {#snapshot_sampling_precision}\n",
    "\n",
    "Next, scores can be plotted for analysis. Here, we wrote the `create_histograms` function to run `imp_sampcon plot_score` so that it plots distributions for various scores of interest. Each of these plots are saved to `histograms{state}_{time}/{score}.png`, where score is an object listed in the `score_list`. These plots are useful for debugging the modeling protocol, and should appear roughly Gaussian.\n",
    "\n",
    "```python\n",
    "# 4 calling create_histograms and related parameters\n",
    "score_list = [\n",
    "    'Total_Score',\n",
    "    'ConnectivityRestraint_Score',\n",
    "    'ExcludedVolumeSphere_Score',\n",
    "    'GaussianEMRestraint_None',\n",
    "    'GaussianEMRestraint_None_CCC'\n",
    "] # list of histograms we want to create in each histograms{state}_{time} directory\n",
    "\n",
    "create_histograms(state_dict, main_dir, score_list)\n",
    "print(\"create_histograms is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "```\n",
    "\n",
    "We then check the number of models in each sampling run though our function, `count_rows_and_generate_report`, which writes the `independent_samples_stat.txt` file. Empirically, we have found that ensuring the overall number of models in each independent sample after filtering is roughly equal serves a good first check on sampling convergence.\n",
    "\n",
    "```python\n",
    "# 5 calling count_rows_and_generate_report\n",
    "count_rows_and_generate_report(state_dict)\n",
    "print(\"count_rows_and_generate_report is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "```\n",
    "\n",
    "Next, we write the density range dictionaries, which are output as `{state}_{time}_density_ranges.txt`. These dictionaries label each protein in each snapshot model, which will be passed into `imp_sampcon` to calculate the localization density of each protein.\n",
    "\n",
    "```python\n",
    "# 6 calling create_density_dictionary:\n",
    "create_density_dictionary_files(state_dict, main_dir)\n",
    "print(\"create_density_dictionary is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "```\n",
    "\n",
    "Next, we run `imp_sampcon exhaust` on each snapshot model. This code performs checks on the exhaustiveness of the sampling. Specifically it analyzes the convergence of the model score, whether the two model sets were drawn from the same distribution, and whether each structural cluster includes models from each sample proportionally to its size. The output for each snapshot model is written out to the `exhaust_{state}_{time}` folder.\n",
    "\n",
    "```python\n",
    "# 7 calling exhaust\n",
    "exhaust(state_dict, main_dir)\n",
    "print(\"exhaust is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "```\n",
    "\n",
    "Plots for determining the sampling precision are shown below for a single snapshot model, 1_2min. (a) Tests the convergence of the lowest scoring model (`snapshot_{state}_{time}.Top_Score_Conv.pdf`). Error bars represent standard deviations of the best scores, estimated by selecting different subsets of models 10 times. The light-blue line indicates a lower bound reference on the total score. (b) Tests that the scores of two independently sampled models come from the same distribution (`snapshot_{state}_{time}.Score_Dist.pdf`). The difference between the two distributions, as measured by the KS test statistic (D) and KS test p-value (p) indicates that the difference is both statistically insignificant (p>0.05) and small in magnitude (D<0.3). (c) Determines the structural precision of a snapshot model (`snapshot_{state}_{time}.ChiSquare.pdf`). RMSD clustering is performed at 1 Å intervals until the clustered population (% clustered) is greater than 80%, and either the χ<sup>2</sup> p-value is greater than 0.05 or Cramer’s V is less than 0.1. The sampling precision is indicated by the dashed black line. (d) Populations from sample 1 and sample 2 are shown for each cluster (`snapshot_{state}_{time}.Cluster_Population.pdf`).\n",
    "\n",
    "<img src=\"images/Snapshot_Exhaust.png\" width=\"600px\" />\n",
    "\n",
    "Further structural analysis can be calculated by using the `cluster.*` files. The `cluster.*.{sample}.txt` files contain the model number for the models in that cluster, where `{sample}` indicates which round of sampling the models came from. The `cluster.*` folder contains an RMF for centroid model of that cluster, along with the localization densities for each protein. The localization densities of each protein from each independent sampling can be compared to ensure independent samplings produce the same results.\n",
    "\n",
    "Ideally, each of these plots should be checked for each snapshot model. As a way to summarize the output of these checks, we can gather the results of the KS test and the sampling precision test for all snapshot models. This is done by running `extract_exhaust_data` and `save_exhaust_data_as_png`, which write `KS_sampling_precision_output.txt` and `KS_sampling_precision_output.png`, respectively.\n",
    "\n",
    "```python\n",
    "# 8 calling extract_exhaust_data\n",
    "extract_exhaust_data(state_dict)\n",
    "print(\"extract_exhaust_data is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "# 9 calling save_exhaust_data_as_png\n",
    "save_exhaust_data_as_png()\n",
    "print(\"save_exhaust_data_as_png is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "```\n",
    "\n",
    "These codes write a table that include the KS two sample test statistic (D), the KS test p-value, and the sampling precision for each snapshot model, which is replotted below.\n",
    "\n",
    "<img src=\"images/Snapshot_sampling.png\" width=\"300px\" />\n",
    "\n",
    "## Visualizing models {#snapshot_visualization}\n",
    "\n",
    "The resulting RMF files and localization densities from this analysis can be viewed in [UCSF Chimera](https://www.rbvi.ucsf.edu/chimera/) (version>=1.13) or [UCSF ChimeraX](https://www.cgl.ucsf.edu/chimerax/).\n",
    "\n",
    "Here, we plotted each centroid model (A - blue, B - orange, and C - purple) from the most populated cluster for each snapshot model and compared that model to the experimental EM profile (gray).\n",
    "\n",
    "<img src=\"images/static_snapshots_noCC.png\" width=\"300px\" />\n",
    "\n",
    "Finally, now that snapshot models were assessed, we can perform modeling of a trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2546f-7c6f-4146-8c9b-ee143fcead6e",
   "metadata": {},
   "source": [
    "Modeling of a Trajectory {#trajectories}\n",
    "====================================\n",
    "\n",
    "Here, we describe the final modeling problem in our composite workflow, how to build models of trajectory models using IMP.\n",
    "\n",
    "# Trajectory modeling step 1: gathering of information {#trajectories1}\n",
    "\n",
    "We begin trajectory modeling with the first step of integrative modeling, gathering information. Trajectory modeling utilizes dynamic information about the bimolecular process. In this case, we utilize heterogeneity models, snapshot models, physical theories, and synthetically generated small-angle X-ray scattering (SAXS) profiles.\n",
    "\n",
    "<img src=\"images/Input_trajectories.png\" width=\"600px\" />\n",
    "\n",
    "Heterogeneity models inform the possible compositional states at each time point and measure how well a compositional state agrees with input information. Snapshot models provide structural models for each heterogeneity model and measure how well those structural models agree with input information about their structure. Physical theories of macromolecular dynamics inform transitions between states. SAXS data informs the size and shape of the assembling complex and is left for validation.\n",
    "\n",
    "# Trajectory modeling step 2: representation, scoring function, and search process {#trajectories2}\n",
    "\n",
    "Trajectory modeling connects alternative snapshot models at adjacent time points, followed by scoring the trajectory models based on their fit to the input information, as described in full [here](https://www.biorxiv.org/content/10.1101/2024.08.06.606842v1.abstract).\n",
    "\n",
    "## Background behind integrative spatiotemporal modeling\n",
    "\n",
    "### Representing the model {#trajectory_representation}\n",
    "\n",
    "We choose to represent dynamic processes as a trajectory of snapshot models, with one snapshot model at each time point. In this case, we computed snapshot models at 3 time points (0, 1, and 2 minutes), so a single trajectory model will consist of 3 snapshot models, one at each 0, 1, and 2 minutes. The modeling procedure described here will produce a set of scored trajectory models, which can be displayed as a directed acyclic graph, where nodes in the graph represent the snapshot model and edges represent connections between snapshot models at neighboring time points.\n",
    "\n",
    "### Scoring the model {#trajectory_scoring}\n",
    "\n",
    "To score trajectory models, we incorporate both the scores of individual snapshot models, as well as the scores of transitions between them. Under the assumption that the process is Markovian (*i.e.* memoryless), the weight of a trajectory model takes the form:\n",
    "\n",
    "$$\n",
    "W(\\chi) \\propto   \\displaystyle\\prod^{T}_{t=0} P( X_{N,t}, N_{t} | D_{t}) \\cdot \\displaystyle\\prod^{T-1}_{t=0} W(X_{N,t+1},N_{t+1} | X_{N,t},N_{t}, D_{t,t+1}),\n",
    "$$\n",
    "\n",
    "where $t$ indexes times from 0 until the final modeled snapshot ($T$); $P(X_{N,t}, N_{t} | D_{t})$ is the snapshot model score; and \\f$W(X_{N,t+1},N_{t+1} | X_{N,t},N_{t}, D_{t,t+1})\\f$ is the transition score. Trajectory model weights ($W(\\chi)$) are normalized so that the sum of all trajectory models' weights is 1.0. Transition scores are currently based on a simple metric that either allows or disallows a transition. Transitions are only allowed if all proteins in the first snapshot model are included in the second snapshot model. In the future, we hope to include more detailed transition scoring terms, which may take into account experimental information or physical models of macromolecular dynamics.\n",
    "\n",
    "### Searching for good scoring models {#trajectory_searching}\n",
    "\n",
    "Trajectory models are constructed by enumerating all connections between adjacent snapshot models and scoring these trajectory models according to the equation above. This procedure results in a set of weighted trajectory models.\n",
    "\n",
    "## Computing trajectory models\n",
    "\n",
    "To compute trajectory models, we first copy all necessary files to a new directory, `data`. These files are (i) `{state}_{time}.config` files, which include the subcomplexes that are in each state, (ii) `{state}_{time}_scores.log`, which is a list of all scores of all structural models in that snapshot model, and (iii) `exp_comp{prot}.csv`, which is the experimental copy number for each protein (`{prot}`) as a function of time. Here, we copy files related to the snapshots (`*.log` files) from the `modeling` directory, as we skipped computing snapshots due to the computational expense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb887efe-0630-47b6-9bf1-85fa216a6816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_scores(fileA, fileB, outputFile):\n",
    "    \"\"\"\n",
    "    For each function merges scoresA.txt and scoresB.txt into {state}_{time}_scores.log\n",
    "\n",
    "    :param fileA: path to scoresA.txt\n",
    "    :param fileB: path to scoresB.txt\n",
    "    :param outputFile: path to output merged .log file named {state}_{time}_scores.log for each snapshot.\n",
    "    This type of .log file is used in crete_DAG to generate trajectory model.\n",
    "    \"\"\"\n",
    "    # open both files, so data can be extracted\n",
    "    with open(fileA, 'r') as file_a:\n",
    "        data_a = file_a.readlines()\n",
    "\n",
    "    with open(fileB, 'r') as file_b:\n",
    "        data_b = file_b.readlines()\n",
    "\n",
    "    # Merge the content of both files\n",
    "    merged_data = data_a + data_b\n",
    "\n",
    "    # Write the merged content into the output file\n",
    "    with open(outputFile, 'w') as output:\n",
    "        output.writelines(merged_data)\n",
    "\n",
    "def create_data_and_copy_files(state_dict, custom_source_dir1 = None, custom_source_dir2 = None, custom_source_dir3 = None):\n",
    "    \"\"\"\n",
    "    Copies three types of files important to generate trajectory models:\n",
    "    -.config files created with start_sim.py in Snapshot_Modeling (source_dir1)\n",
    "    -time-dependent stoichiometry data for each timepoint. Data should be presented in .csv file. With this function all\n",
    "    csv file in source_dir2 will be copied. These .csv files will be used in the exp_comp dictionary in create_DAG\n",
    "    function\n",
    "    -scoresA and scoresB for each snapshot created with imp sampcon exhaust\n",
    "    (source_dir1 + snapshot + good_scoring_models) are merged into total score .txt using merge_scores helper function.\n",
    "    All copied files are gathered in newly created './data/' directory, where everything is prepared for create_DAG\n",
    "    function.\n",
    "\n",
    "\n",
    "    :param state_dict (dict): state_dict: dictionary that defines the spatiotemporal model.\n",
    "           The keys are strings that correspond to each time point in the\n",
    "           stepwise temporal process. Keys should be ordered according to the\n",
    "           steps in the spatiotemporal process. The values are integers that\n",
    "           correspond to the number of possible states at that timepoint.\n",
    "    :param custom_source_dir1 (optional - str): Custom path to heterogeneity modeling dir (heterogeneity_modeling.py),\n",
    "    to copy .config files\n",
    "    :param custom_source_dir2 (optional - str): Custom path to stoichiometry data dir\n",
    "    :param custom_source_dir3 (optional - str): Custom path to snapshot modeling dir (start_sim.py), to copy .config\n",
    "    files and to access scoresA/scoresB (custom_source_dir3 + snapshot{state}_{time} + 'good_scoring_models')\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the destination directory if it does not exist (./data/). Here all the\n",
    "    destination_dir = './data/'\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "    # Path to heterogeneity modeling dir\n",
    "    if custom_source_dir1:\n",
    "        source_dir1 = custom_source_dir1\n",
    "    else:\n",
    "        source_dir1 = '../../Heterogeneity/Heterogeneity_Modeling/'\n",
    "\n",
    "    # Path to stoichiometry data dir\n",
    "    if custom_source_dir2:\n",
    "        source_dir2 = custom_source_dir2\n",
    "    else:\n",
    "        source_dir2 = '../../Input_Information/gen_FCS/'\n",
    "\n",
    "    # Path to snapshot modeling dir\n",
    "    if custom_source_dir3:\n",
    "        source_dir3 = custom_source_dir3\n",
    "    else:\n",
    "        source_dir3 = '../../Snapshots/Snapshots_Modeling/'\n",
    "\n",
    "    # Copy all .config files from the first source directory to the destination directory\n",
    "    try:\n",
    "        for file_name in os.listdir(source_dir1):\n",
    "            if file_name.endswith('.config'):\n",
    "                full_file_name = os.path.join(source_dir1, file_name)\n",
    "                if os.path.isfile(full_file_name):\n",
    "                    shutil.copy(full_file_name, destination_dir)\n",
    "        print(\".config files are copied\")\n",
    "    except Exception as e:\n",
    "        print(f\".config files cannot be copied. Try do do it manually. Reason for Error: {e}\")\n",
    "\n",
    "    # Copy all .csv stoichiometry files from the second source directory to the destination directory\n",
    "    try:\n",
    "        for file_name in os.listdir(source_dir2):\n",
    "            if file_name.endswith('.csv'):\n",
    "                full_file_name = os.path.join(source_dir2, file_name)\n",
    "                if os.path.isfile(full_file_name):\n",
    "                    shutil.copy(full_file_name, destination_dir)\n",
    "        print(\".csv stoichiometry files are copied\")\n",
    "    except Exception as e:\n",
    "        print(f\".csv stoichiometry files cannot be copied. Try do do it manually. Reason for Error: {e}\")\n",
    "\n",
    "    # Copy scoresA and scoresB from the snapshot_{state}_{time} directories and first source directory path\n",
    "    for time in state_dict.keys():\n",
    "        for state in range(1, state_dict[time] + 1):\n",
    "            dir_name = f\"snapshot{state}_{time}\"\n",
    "            good_scoring_path = \"good_scoring_models\"\n",
    "            file_a = os.path.join(source_dir3, dir_name, good_scoring_path, \"scoresA.txt\")\n",
    "            file_b = os.path.join(source_dir3, dir_name, good_scoring_path, \"scoresB.txt\")\n",
    "            output_file = os.path.join(destination_dir, f\"{state}_{time}_scores.log\") # name of the output file\n",
    "\n",
    "            try:\n",
    "                # Ensure the directory exists before try to read/write files\n",
    "                if os.path.exists(file_a) and os.path.exists(file_b):\n",
    "                    merge_scores(file_a, file_b, output_file) # call helper function to merge files\n",
    "                    print(f\"Scores for snapshot{state}_{time} have been merged and saved\")\n",
    "                else:  # many things can go wrong here, so it is good to know where is the problem\n",
    "                    print(f\"Path doesn't exist: {source_dir3}\")\n",
    "                    print(f\"Files not found in directory: {dir_name}\")\n",
    "                    print(f\"Files not found in directory: {file_a}\")\n",
    "                    print(f\"Files not found in directory: {file_b}\")\n",
    "                    print(f\"Output directory doesn't exist: {destination_dir}\")\n",
    "            except Exception as e:\n",
    "                print(f\"total scores files cannot be copied of merged. Reason for Error: {e}\")\n",
    "\n",
    "# copy all the relevant files for create_DAG\n",
    "# it is important that everything starts from main dir\n",
    "main_dir = os.getcwd()\n",
    "os.chdir(main_dir)\n",
    "state_dict = {'0min': 3, '1min': 3, '2min': 1}\n",
    "create_data_and_copy_files(state_dict, custom_source_dir1=main_dir, custom_source_dir2='../../../modeling/Input_Information/gen_FCS/', custom_source_dir3='../../../modeling/Snapshots/Snapshots_Modeling/')\n",
    "\n",
    "# then trajectory model is created based on the all copied data\n",
    "expected_subcomplexes = ['A', 'B', 'C']\n",
    "exp_comp = {'A': 'exp_compA.csv', 'B': 'exp_compB.csv', 'C': 'exp_compC.csv'}\n",
    "input = './data/'\n",
    "output = \"../output/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
