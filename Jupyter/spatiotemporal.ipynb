{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a3dfff8-9d60-418c-a5fd-a4adf7a121b1",
   "metadata": {
    "id": "4a3dfff8-9d60-418c-a5fd-a4adf7a121b1"
   },
   "source": [
    "IMP spatiotemporal tutorial<a id=\"notebook\"></a>\n",
    "========\n",
    "\n",
    "# Introduction<a id=\"notebook_introduction\"></a>\n",
    "\n",
    "Biomolecules are constantly in motion; therefore, a complete depiction of their function must include their dynamics instead of just static structures. We have developed an integrative spatiotemporal approach to model dynamic systems.\n",
    "\n",
    "Our approach applies a composite workflow, consisting of three modeling problems to compute (i) heterogeneity models, (ii) snapshot models, and (iii) a trajectory model.\n",
    "Heterogeneity models describe the possible biomolecular compositions of the system at each time point. Optionally, other auxiliary variables can be considered, such as the coarse location in the final state when modeling an assembly process.\n",
    "For each heterogeneity model, one snapshot model is produced. A snapshot model is a set of alternative standard static integrative structure models based on the information available for the corresponding time point.\n",
    "Then, a set of trajectories ranked by their agreement with input information is computed by connecting alternative snapshot models at adjacent time points (*i.e.*, the “trajectory model”). This trajectory model can be scored based on both the scores of static structures and the transitions between them, allowing for the creation of trajectories that are in agreement with the input information by construction.\n",
    "\n",
    "If you use this tutorial or its accompanying method, please site the corresponding publications:\n",
    "\n",
    "- [Latham, AP; Zhang, W; Tempkin, JOB; Otsuka, S; Ellenberg, J; Sali, A. Integrative spatiotemporal modeling of biomolecular processes: application to the assembly of the Nuclear Pore Complex, Proc. Natl. Acad. Sci. U.S.A., 2025, 122, e2415674122.](https://www.pnas.org/doi/10.1073/pnas.2415674122)\n",
    "- [Latham, AP; Rožič, M; Webb, BM; Sali, A. Tutorial on integrative spatiotemporal modeling by integrative modeling platform, Protein Sci., 2025, 34, e70107.](https://onlinelibrary.wiley.com/doi/10.1002/pro.70107)\n",
    "\n",
    "\n",
    "# Integrative spatiotemporal modeling workflow<a id=\"notebook_steps\"></a>\n",
    "\n",
    "In general, integrative modeling proceeds through three steps (i. gathering information; ii. choosing the model representation, scoring alternative models, and searching for good scoring models; and iii. assessing the models). In integrative spatiotemporal modeling, these three steps are repeated for each modeling problem in the composite workflow (i. modeling of heterogeneity, ii. modeling of snapshots, and iii. modeling of a trajectory).\n",
    "\n",
    "<img src=\"https://github.com/salilab/imp_spatiotemporal_tutorial/blob/main/Jupyter/images/Overview.png?raw=1\" width=\"300px\" />\n",
    "\n",
    "This tutorial will walk you through the creation of a spatiotemporal model for the  hypothetical assembly mechanism of the Bmi1/Ring1b-UbcH5c complex. We note that all experimental data besides the static structure used in this study is purely hypothetical, and, thus, the model should not be interpreted to be meaningful about the actual assembly mechanism of the complex.\n",
    "\n",
    "Finally, this notebook is intended to present an abbreviated version of this protocol, with the computationally expensive steps abbreviated. A more complete version of this tutorial can be found as a series of python scripts at https://github.com/salilab/imp_spatiotemporal_tutorial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b63e9fd",
   "metadata": {
    "id": "7b63e9fd"
   },
   "outputs": [],
   "source": [
    "# General imports for the tutorial\n",
    "import sys, os, glob, shutil, random, ast, re\n",
    "import IMP\n",
    "import IMP.atom\n",
    "import RMF\n",
    "import subprocess\n",
    "import IMP.rmf\n",
    "from IMP.spatiotemporal import prepare_protein_library\n",
    "import IMP.spatiotemporal as spatiotemporal\n",
    "from IMP.spatiotemporal import analysis\n",
    "from IPython.display import display\n",
    "from IPython.display import IFrame\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d93b4-3b97-4187-867c-977f8353f4aa",
   "metadata": {
    "id": "dc9d93b4-3b97-4187-867c-977f8353f4aa"
   },
   "source": [
    "Modeling of heterogeneity<a id=\"notebook_heterogeneity\"></a>\n",
    "====================================\n",
    "\n",
    "Here, we describe the first modeling problem in our composite workflow, how to build models of heterogeneity using IMP. In this tutorial, heterogeneity modeling only includes protein copy number; however, in general, other types of information, such as the coarse location in the final state, could also be included in heterogeneity models.\n",
    "\n",
    "# Heterogeneity modeling step 1: gathering of information<a id=\"notebook_heterogeneity1\"></a>\n",
    "\n",
    "We begin heterogeneity modeling with the first step of integrative modeling, gathering information. Heterogeneity modeling will rely on copy number information about the complex. In this case, we utilize the X-ray crystal structure of the fully assembled Bmi1/Ring1b-UbcH5c complex from the protein data bank (PDB), and synthetically generated protein copy numbers during the assembly process, which could be generated from experiments such as flourescence correlation spectroscopy (FCS).\n",
    "\n",
    "<img src=\"https://github.com/salilab/imp_spatiotemporal_tutorial/blob/main/Jupyter/images/Input_heterogeneity.png?raw=1\" width=\"600px\" />\n",
    "\n",
    "The PDB structure of the complex informs the final state of our model and constrains the maximum copy number for each protein, while the protein copy number data gives time-dependent information about the protein copy number in the assembling complex.\n",
    "\n",
    "# Heterogeneity modeling step 2: representation, scoring function, and search process<a id=\"notebook_heterogeneity2\"></a>\n",
    "\n",
    "Next, we represent, score and search for heterogeneity models models. A single heterogeneity model is a set of protein copy numbers, scored according to its fit to experimental copy number data at that time point. As ET and SAXS data, are only available at 0 minutes, 1 minute, and 2 minutes, we choose to create heterogeneity models at these three time points. We then use `prepare_protein_library`, to calculate the protein copy numbers for each heterogeneity model and to use the topology file of the full complex (`spatiotemporal_topology.txt`) to generate a topology file for each corresponding snapshot model. The choices made in this topology file are important for the representation, scoring function, and search process for snapshot models, and are discussed later. For heterogeneity modeling, we choose to model 3 protein copy numbers at each time point, and restrict the final time point to have the same protein copy numbers as the PDB structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b78a015-32e5-4701-8c6f-df14863ba9ce",
   "metadata": {
    "id": "6b78a015-32e5-4701-8c6f-df14863ba9ce"
   },
   "outputs": [],
   "source": [
    "main_dir = os.getcwd()\n",
    "os.chdir(main_dir)\n",
    "# parameters for prepare_protein_library:\n",
    "times = [\"0min\", \"1min\", \"2min\"]\n",
    "exp_comp = {'A': '../modeling/Input_Information/gen_FCS/exp_compA.csv',\n",
    "            'B': '../modeling/Input_Information/gen_FCS/exp_compB.csv',\n",
    "            'C': '../modeling/Input_Information/gen_FCS/exp_compC.csv'}\n",
    "expected_subcomplexes = ['A', 'B', 'C']\n",
    "template_topology = '../modeling/Heterogeneity/Heterogeneity_Modeling/spatiotemporal_topology.txt'\n",
    "template_dict = {'A': ['Ubi-E2-D3'], 'B': ['BMI-1'], 'C': ['E3-ubi-RING2']}\n",
    "nmodels = 3\n",
    "\n",
    "# calling prepare_protein_library\n",
    "prepare_protein_library.prepare_protein_library(times, exp_comp, expected_subcomplexes, nmodels,\n",
    "                                                template_topology=template_topology, template_dict=template_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56fbe48-da12-412e-ac2e-dca673e04a43",
   "metadata": {
    "id": "b56fbe48-da12-412e-ac2e-dca673e04a43"
   },
   "source": [
    "From the output of `prepare_protein_library`, we see that there are 3 heterogeneity models at each time point (it is possible to have more heterogeneity models than copy numbers if multiple copies of the protein exist in the complex). For each heterogeneity model, we see 2 files:\n",
    "- *.config, a file with a list of proteins represented in the heterogeneity model.\n",
    "- *_topol.txt, a topology file for snapshot modeling corresponding to this heterogeneity model.\n",
    "\n",
    "# Heterogeneity modeling step 3: assessment<a id=\"notebook_heterogeneity_assess\"></a>\n",
    "\n",
    "Now, we have a variety of heterogeneity models. In general, there are four ways to assess a model: estimate the sampling precision, compare the model to data used to construct it, validate the model against data not used to construct it, and quantify the precision of the model. Here, we will focus specifically on comparing the model to experimental data, as other assessments will be performed later, when the trajectory model is assessed.\n",
    "\n",
    "Specifically, we can plot the modeled and experimental copy numbers simultaneously for each protein, as shown below for proteins A, B, and C.\n",
    "\n",
    "From these plots, we observe that the range of possible experimental copy numbers are well sampled by the heterogeneity models, indicating that we are prepared for snapshot modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c484872c-6db1-4643-9362-ce7baadd00f3",
   "metadata": {
    "id": "c484872c-6db1-4643-9362-ce7baadd00f3"
   },
   "outputs": [],
   "source": [
    "# colors\n",
    "blue1=np.array([0,0,128])/255\n",
    "grey=np.array([188,  188,  188])/255\n",
    "\n",
    "# Import experimental data\n",
    "expA=np.loadtxt(\"../modeling/Input_Information/gen_FCS/A_v2.txt\")\n",
    "expB=np.loadtxt(\"../modeling/Input_Information/gen_FCS/B_v2.txt\")\n",
    "expC=np.loadtxt(\"../modeling/Input_Information/gen_FCS/C_v2.txt\")\n",
    "\n",
    "# Import heterogeneity models\n",
    "model0=np.loadtxt(\"0min.txt\")\n",
    "model1=np.loadtxt(\"1min.txt\")\n",
    "model2=np.loadtxt(\"2min.txt\")\n",
    "\n",
    "# Convert heterogeneity models from copy number as a function of time to copy number as a function of protein\n",
    "# There are 7 total models. First row is the copy number of that model. 2nd row is the time of that model, which we add manually\n",
    "# For protein A\n",
    "modelA=np.zeros((7,2))\n",
    "modelA[0:3,0] = model0[:,0]\n",
    "modelA[0:3,1] = 0\n",
    "modelA[3:6, 0] = model1[:, 0]\n",
    "modelA[3:6, 1] = 1\n",
    "modelA[6, 0] = model2[0]\n",
    "modelA[6, 1] = 2\n",
    "\n",
    "# For protein B\n",
    "modelB = np.zeros((7, 2))\n",
    "modelB[0:3, 0] = model0[:, 1]\n",
    "modelB[0:3, 1] = 0\n",
    "modelB[3:6, 0] = model1[:, 1]\n",
    "modelB[3:6, 1] = 1\n",
    "modelB[6, 0] = model2[1]\n",
    "modelB[6, 1] = 2\n",
    "\n",
    "# For protein C\n",
    "modelC = np.zeros((7, 2))\n",
    "modelC[0:3, 0] = model0[:, 2]\n",
    "modelC[0:3, 1] = 0\n",
    "modelC[3:6, 0] = model1[:, 2]\n",
    "modelC[3:6, 1] = 1\n",
    "modelC[6, 0] = model2[2]\n",
    "modelC[6, 1] = 2\n",
    "\n",
    "# Plot data\n",
    "fig, ((ax1), (ax2), (ax3)) = plt.subplots(3, 1, figsize=(5,8))\n",
    "ax1.errorbar(expA[:,0], expA[:,1], yerr=expA[:,2], color=grey)\n",
    "ax1.plot(modelA[:,1], modelA[:,0], 'o', color=blue1)\n",
    "ax1.set_ylabel('Copy number [A]')\n",
    "ax1.set_xlabel('Time [min]')\n",
    "ax1.set_xlim(-0.2,2.7)\n",
    "ax1.set_ylim(-1,2.5)\n",
    "ax1.legend(['model', 'experiment'], loc='upper left')\n",
    "ax2.errorbar(expB[:,0], expB[:,1], yerr=expB[:,2], color=grey)\n",
    "ax2.plot(modelB[:,1], modelB[:,0], 'o', color=blue1)\n",
    "ax2.set_ylabel('Copy number [B]')\n",
    "ax2.set_xlabel('Time [min]')\n",
    "ax2.set_xlim(-0.2, 2.7)\n",
    "ax2.set_ylim(-0.5, 3)\n",
    "ax2.legend(['model', 'experiment'], loc='upper left')\n",
    "ax3.errorbar(expC[:,0], expC[:,1], yerr=expC[:,2], color=grey)\n",
    "ax3.plot(modelC[:,1], modelC[:,0], 'o', color=blue1)\n",
    "ax3.set_ylabel('Copy number [C]')\n",
    "ax3.set_xlabel('Time [min]')\n",
    "ax3.set_xlim(-0.2, 2.7)\n",
    "ax2.set_ylim(-0.5, 3)\n",
    "ax3.legend(['model', 'experiment'], loc='upper left')\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496f2687-bb05-4f51-8eab-a1170f6ac5fa",
   "metadata": {
    "id": "496f2687-bb05-4f51-8eab-a1170f6ac5fa"
   },
   "source": [
    "Modeling of snapshots<a id=\"notebook_snapshots\"></a>\n",
    "====================================\n",
    "\n",
    "Here, we describe the second modeling problem in our composite workflow, how to build static snapshot models using IMP. We note that this process is similar to previous tutorials of [actin](https://integrativemodeling.org/tutorials/actin/) and [RNA PolII](https://integrativemodeling.org/tutorials/rnapolii_stalk/).\n",
    "\n",
    "# Snapshot modeling step 1: gathering of information<a id=\"notebook_snapshots1\"></a>\n",
    "\n",
    "We begin snapshot modeling with the first step of integrative modeling, gathering information. Snapshot modeling utilizes structural information about the complex. In this case, we utilize heterogeneity models, the X-ray crystal structure of the fully assembled Bmi1/Ring1b-UbcH5c complex from the protein data bank (PDB), synthetically generated electron tomography (ET) density maps during the assembly process, and physical principles.\n",
    "\n",
    "<img src=\"https://github.com/salilab/imp_spatiotemporal_tutorial/blob/main/Jupyter/images/Input_snapshot.png?raw=1\" width=\"600px\" />\n",
    "\n",
    "The heterogeneity models inform protein copy numbers for the snapshot models. The PDB structure of the complex informs the structure of the individual proteins. The time-dependent ET data informs the size and shape of the assembling complex. Physical principles inform connectivity and excluded volume.\n",
    "\n",
    "# Snapshot modeling step 2: representation, scoring function, and search process<a id=\"notebook_snapshots2\"></a>\n",
    "\n",
    "Next, we represent, score and search for snapshot models. This step is quite computationally expensive. Therefore, we will not run the entire sampling protocol in this notebook, though the scripts are available in `modeling/Snapshots/Snapshots_Modeling/`. This folder includes `static_snapshot.py`, which uses IMP to represent, score, and search for a single static snapshot model, and `start_sim.py`, which automates the creation of a snapshot model for each heterogeneity model. Here, we will simply run two abbreviated sampling runs on a single snapshot model, similar to two runs of `static_snapshot.py`.\n",
    "\n",
    "## Modeling one snapshot<a id=\"autotoc1v1\"></a>\n",
    "Here, we will describe one instance of sampling a snapshot model.\n",
    "\n",
    "### Representing the model<a id=\"notebook_snapshot_representation\"></a>\n",
    "\n",
    "We begin by representing the data and the model. In general, the *representation* of a system is defined by all the variables that need to be determined.\n",
    "\n",
    "For our model of a protein complex, we use a combination of two representations. The first is a series of *spherical beads*, which can correspond to portions of the biomolecules of interest, such as atoms or groups of atoms. The second is a series of *3D Gaussians*, which help calculate the overlap between our model and the density from ET data.\n",
    "\n",
    "Beads and Gaussians in our model belong to either a *rigid body* or *flexible string*. The positions of all beads and Gaussians in a single rigid body are constrained during sampling and do not move relative to each other. Meanwhile, flexible beads can move freely during sampling, but are restrained by sequence connectivity.\n",
    "\n",
    "To begin, we built a topology file with the representation for the model of the complete system, `spatiotemporal_topology.txt`, located in `Heterogeneity/Heterogeneity_Modeling/`. This complete topology was used as a template to build topologies for each snapshot model. Based on our observation of the structure of the complex, we chose to represent each protein with at least 2 separate rigid bodies, and left the first 28 residues of protein C as flexible beads. Rigid bodies were described with 1 bead for every residue, and 10 residues per Gaussian. Flexible beads were described with 1 bead for every residue and 1 residue per Gaussian. A more complete description of the options available in topology files is available in the the [TopologyReader](https://integrativemodeling.org/2.22.0/doc/ref/classIMP_1_1pmi_1_1topology_1_1TopologyReader.html) documentation.\n",
    "\n",
    "```\n",
    "|molecule_name | color | fasta_fn | fasta_id | pdb_fn | chain | residue_range | pdb_offset | bead_size | em_residues_per_gaussian | rigid_body | super_rigid_body | chain_of_super_rigid_bodies |\n",
    "\n",
    "|Ubi-E2-D3|blue|3rpg.fasta.txt|Ubi-E2-D3|3rpg.pdb|A|-1,18|2|1|10|1|1||\n",
    "|Ubi-E2-D3|blue|3rpg.fasta.txt|Ubi-E2-D3|3rpg.pdb|A|19,147|2|1|10|2|1||\n",
    "|BMI-1|red|3rpg.fasta.txt|BMI-1|3rpg.pdb|B|3,83|-2|1|10|3|2||\n",
    "|BMI-1|red|3rpg.fasta.txt|BMI-1|3rpg.pdb|B|84,101|-2|1|10|4|2||\n",
    "|E3-ubi-RING2|green|3rpg.fasta.txt|E3-ubi-RING2|BEADS|C|16,44|-15|1|1|5|3||\n",
    "|E3-ubi-RING2|green|3rpg.fasta.txt|E3-ubi-RING2|3rpg.pdb|C|45,116|-15|1|10|6|3||\n",
    "```\n",
    "\n",
    "Next, we must prepare the script to read in this topology file. We begin by defining the input variables, `state` and `time`, which define which heterogeneity model to use, as well as the paths to other pieces of input information.\n",
    "\n",
    "We then build the system (`IMP.pmi.topology.TopologyReader` and `IMP.pmi.macros.BuildSystem`), using the topology file, described above.\n",
    "\n",
    "Next, we prepare for later sampling steps by setting which Monte Carlo moves will be performed (`bs.execute_macro`). Rotation (`rot`) and translation (`trans`) parameters are separately set for super rigid bodies (`srb`), rigid bodies (`rb`), and beads (`bead`).\n",
    "\n",
    "### Scoring the model<a id=\"notebook_snapshot_scoring\"></a>\n",
    "\n",
    "After building the model representation, we choose a scoring function to score the model based on input information. This scoring function is represented as a series of restraints that serve as priors. Our scoring function includes:\n",
    "\n",
    "* connectivity restraints (`IMP.pmi.restraints.stereochemistry.ConnectivityRestraint`), which restrains beads adjacent in sequence to be close in 3D space.\n",
    "* excluded volume restraint (`IMP.pmi.restraints.stereochemistry.ExcludedVolumeSphere`), which restrains beads to minimize their spatial overlap.\n",
    "* ET restraints (`IMP.pmi.restraints.em.GaussianEMRestraint`), which bias our models based on their fit to ET density maps. Both the experimental map and the forward protein density are represented as Gaussian mixture models (GMMs) to speed up scoring. The score is based on the log of the correlation coefficient between the experimental density and the forward protein density.\n",
    "\n",
    "### Searching for good scoring models<a id=\"notebook_snapshot_searching\"></a>\n",
    "\n",
    "After building a scoring function that scores alternative models based on their fit to the input information, we aim to search for good scoring models. For complicated systems, stochastic sampling techniques such as Monte Carlo (MC) sampling are often the most efficient way to compute good scoring models. For the tutorial manuscript, we generated a random initial configuration and then perform temperature replica exchange MC sampling with 16 temperatures from different initial configurations. By performing multiple runs of replica exchange MC from different initial configurations, we can later ensure that our sampling is sufficiently converged. Here, we perform 2 shorter runs with a single replica for computation efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d85ab",
   "metadata": {
    "id": "ea3d85ab"
   },
   "outputs": [],
   "source": [
    "# import PMI for sampling\n",
    "import IMP.pmi\n",
    "import IMP.pmi.io\n",
    "import IMP.pmi.topology\n",
    "import IMP.pmi.macros\n",
    "import IMP.pmi.restraints\n",
    "import IMP.pmi.restraints.stereochemistry\n",
    "import IMP.pmi.restraints.em\n",
    "import IMP.pmi.dof\n",
    "\n",
    "# Running parameters to access correct path of ET_data for EM restraint\n",
    "# and topology file for certain {state}_{time}_topol.txt\n",
    "state = '1'\n",
    "time = '2min'\n",
    "\n",
    "os.chdir(main_dir)\n",
    "snapshot_dir=os.path.join(main_dir, f'snapshot{state}_{time}')\n",
    "if not os.path.exists(snapshot_dir):\n",
    "    os.system('mkdir '+snapshot_dir)\n",
    "os.chdir(snapshot_dir)\n",
    "seeds=[1,2]\n",
    "for i in range(1,3):\n",
    "    random.seed(seeds[i-1])\n",
    "    run_folder=os.path.join(snapshot_dir, 'run'+str(i))\n",
    "    if not os.path.exists(run_folder):\n",
    "        os.system('mkdir '+run_folder)\n",
    "    os.chdir(run_folder)\n",
    "\n",
    "    # Topology file\n",
    "    topology_file = f\"../../{state}_{time}_topol.txt\"\n",
    "    # Paths to input data for topology file\n",
    "    pdb_dir = \"../../../modeling/Input_Information/PDB/\"\n",
    "    fasta_dir = \"../../../modeling/Input_Information/FASTA/\"\n",
    "    # Path where forward gmms are created with BuildSystem (based ont topology file)\n",
    "    # If gmms exist, they will be used from this folder\n",
    "    forward_gmm_dir = f\"../../../modeling/Snapshots/Snapshots_Modeling/snapshot{state}_{time}/forward_densities/\"\n",
    "    # Path to experimental gmms\n",
    "    exp_gmm_dir= '../../../modeling/Input_Information/ET_data/add_noise/'\n",
    "\n",
    "    # Setting model\n",
    "    mdl = IMP.Model()\n",
    "\n",
    "    # Read the topology file\n",
    "    t = IMP.pmi.topology.TopologyReader(topology_file, pdb_dir=pdb_dir, fasta_dir=fasta_dir, gmm_dir=forward_gmm_dir)\n",
    "\n",
    "\n",
    "    # Create a system from a topology file. Resolution is set on 1.\n",
    "    bs = IMP.pmi.macros.BuildSystem(mdl, resolutions= 1, name= f'Static_snapshots_{state}_{time}')\n",
    "    bs.add_state(t)\n",
    "\n",
    "    #  Macro execution: It gives hierarchy and degrees of freedom (dof).\n",
    "    # In dof we define how much can each (super) rigid body translate and rotate between two adjacent Monte Carlo steps\n",
    "    root_hier, dof = bs.execute_macro(max_rb_trans=1.0,\n",
    "                                  max_rb_rot=0.5, max_bead_trans=2.0,\n",
    "                                  max_srb_trans=1.0, max_srb_rot=0.5)\n",
    "\n",
    "    # Adding Restraints\n",
    "    # Empty list where the data from restraints should be collected\n",
    "    output_objects=[]\n",
    "\n",
    "    # Two common restraints: ConnectivityRestraint and ExcludedVolumeSphere\n",
    "    # ConnectivityRestraint is added for each \"molecule\" separately\n",
    "    for m in root_hier.get_children()[0].get_children():\n",
    "        cr = IMP.pmi.restraints.stereochemistry.ConnectivityRestraint(m)\n",
    "        cr.add_to_model()\n",
    "        output_objects.append(cr)\n",
    "\n",
    "    # Add excluded volume\n",
    "    evr = IMP.pmi.restraints.stereochemistry.ExcludedVolumeSphere(\n",
    "                                     included_objects=[root_hier],\n",
    "                                     resolution=1000)\n",
    "    output_objects.append(evr)\n",
    "\n",
    "    # Applying time-dependent EM restraint. Point to correct gmm / mrc file at each time point\n",
    "    # Path to corresponding .gmm file (and .mrc file)\n",
    "    em_map = exp_gmm_dir + f\"/{time}_noisy.gmm\"\n",
    "\n",
    "    # Create artificial densities from hierarchy\n",
    "    densities = IMP.atom.Selection(root_hier,\n",
    "                 representation_type=IMP.atom.DENSITIES).get_selected_particles()\n",
    "\n",
    "    # Create EM restraint based on these densities\n",
    "    emr = IMP.pmi.restraints.em.GaussianEMRestraint(\n",
    "        densities,\n",
    "        target_fn=em_map, #\n",
    "        slope=0.000001,\n",
    "        scale_target_to_mass=True,\n",
    "        weight=1000)\n",
    "    output_objects.append(emr)\n",
    "\n",
    "\n",
    "    # Generate random configuration\n",
    "    IMP.pmi.tools.shuffle_configuration(root_hier,\n",
    "                                    max_translation=50)\n",
    "\n",
    "    # Add EM restraint and excluded volume restraint to the model\n",
    "    evr.add_to_model()\n",
    "    emr.add_to_model()\n",
    "\n",
    "\n",
    "    # Perform replica exchange sampling\n",
    "    rex=IMP.pmi.macros.ReplicaExchange(mdl,\n",
    "        root_hier=root_hier,\n",
    "        monte_carlo_sample_objects=dof.get_movers(),\n",
    "        global_output_directory='output', # name 'output' is the best for imp sampcon select_good\n",
    "        output_objects=output_objects,\n",
    "        monte_carlo_steps=10, # Number of MC steps between writing frames.\n",
    "        number_of_best_scoring_models=0,\n",
    "        number_of_frames=200) # number of frames to be saved\n",
    "    # In our case, for each snapshot we generated 25000 frames altogether (50*500)\n",
    "    rex.execute_macro()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28562b8",
   "metadata": {
    "id": "d28562b8"
   },
   "source": [
    "After performing sampling, a variety of outputs will be created. These outputs include `.rmf` files, which contain multi-resolution models output by IMP, and `.out` files which contains a variety of information about the run such as the value of the restraints and the MC acceptance rate.\n",
    "\n",
    "## Generalizing modeling to all snapshots<a id=\"notebook_snapshot_combine\"></a>\n",
    "\n",
    "Next, we will describe the process of computing multiple static snapshot models, as performed by running `start_sim.py`.\n",
    "\n",
    "From heterogeneity modeling, we see that there are 3 heterogeneity models at each time point (it is possible to have more snapshot models than copy numbers if multiple copies of the protein exist in the complex), each of which has a corresponding topology file in `Heterogeneity/Heterogeneity_Modeling/`. We wrote a function, `generate_all_snapshots`, which creates a directory for each snapshot model, copies the python script and topology file into that directory, and submits a job script to run sampling. The job script will likely need to be customized for the user's computer or cluster.\n",
    "\n",
    "```python\n",
    "# 1a - parameters for generate_all_snapshots\n",
    "# state_dict - universal parameter\n",
    "state_dict = {'0min': 3, '1min': 3, '2min': 1}\n",
    "\n",
    "main_dir = os.getcwd()\n",
    "topol_dir = os.path.join(os.getcwd(), '../../Heterogeneity/Heterogeneity_Modeling')\n",
    "items_to_copy = ['static_snapshot.py']  # additionally we need to copy only specific topology file\n",
    "# jobs script will likely depend on the user's cluster / configuration\n",
    "job_template = (\"#!/bin/bash\\n#$ -S /bin/bash\\n#$ -cwd\\n#$ -r n\\n#$ -j y\\n#$ -N Tutorial\\n#$ -pe smp 16\\n\"\n",
    "                \"#$ -l h_rt=48:00:00\\n\\nmodule load Sali\\nmodule load imp\\nmodule load mpi/openmpi-x86_64\\n\\n\"\n",
    "                \"mpirun -np $NSLOTS python3 static_snapshot.py {state} {time}\")\n",
    "number_of_runs = 50\n",
    "\n",
    "# 1b - calling generate_all_snapshots\n",
    "generate_all_snapshots(state_dict, main_dir, topol_dir, items_to_copy, job_template, number_of_runs)\n",
    "\n",
    "```\n",
    "\n",
    "# Snapshot modeling step 3: assessment<a id=\"notebook_snapshot_assess\"></a>\n",
    "\n",
    "The above code would create a variety of alternative snapshot models. In general, we would like to assess these models in at least 4 ways: estimate the sampling precision, compare the model to data used to construct it, validate the model against data not used to construct it, and quantify the precision of the model. In this portion of the tutorial, we focus specifically on estimating the sampling precision of the model, while quantitative comparisons between the model and experimental data will be reserved for the final step, when we assess trajectories. Here, we will walk through estimating the sampling precision on our short sampling runs.\n",
    "\n",
    "## Filtering good scoring models<a id=\"notebook_snapshot_filter\"></a>\n",
    "\n",
    "Initially, we want to filter the various alternative structural models to only select those that meet certain parameter thresholds. In this case, we filter the structural models comprising each snapshot model by the median cross correlation with EM data. We note that this filtering criteria is subjective, and developing a Bayesian method to objectively weigh different restraints for filtering remains an interesting future development in integrative modeling.\n",
    "\n",
    "The current filtering procedure involves three steps. In the first step, we look through the `stat.*.out` files to write out the cross correlation with EM data for each model, which, in this case, is labeled column `3`, `GaussianEMRestraint_None_CCC`. In other applications, the column that corresponds to each type of experimental data may change, depending on the scoring terms for each model. For each snapshot model, a new file is written with this data (`{state}_{time}_stat.txt`).\n",
    "\n",
    "In the second step, we want to determine the median value of EM cross correlation for each snapshot model. We wrote `general_rule_calculation` to look through the `general_rule_column` for each `{state}_{time}_stat.txt` file and determine both the median value and the number of structures generated. Here, we print out the results of this calculation, where the median cross correlation value and number of included frames can be examined.\n",
    "\n",
    "In the third step, we use the `imp_sampcon select_good` tool to filter each snapshot model, according to the median value determined in the previous step. For each snapshot model, this function produces a file, `good_scoring_models/model_ids_scores.txt`, which contains the run, replicaID, scores, and sampleID for each model that passes filtering. It also saves RMF files with each model from two independent groups of sampling runs from each snapshot model to `good_scoring_models/sample_A` and `good_scoring_models/sample_B`, writes the scores for the two independent groups of sampling runs to `good_scoring_models/scoresA.txt` and `good_scoring_models/scoresB.txt`, and writes `good_scoring_models/model_sample_ids.txt` to connect each model to its division of sampling run. More information on `imp_sampcon` is available in the analysis portion of the [actin tutorial](https://integrativemodeling.org/tutorials/actin/analysis.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b36fb71",
   "metadata": {
    "id": "1b36fb71"
   },
   "outputs": [],
   "source": [
    "def extract_values_from_file(file_path, keys):\n",
    "    \"\"\"\n",
    "    This function is a helper function which extract from stat file (stat.X.out) all the required values associated\n",
    "    with keys provided in the 'keys_to_extract' parameter.\n",
    "\n",
    "    :param file_path (str): path to each stat.X.out from which data should be extracted\n",
    "    :param keys (list of int): 'keys_to_extract' parameter from the extracting_stat_files function.\n",
    "    Look at the header of stat.X.out file to find which dictionary key corresponds to which variable.\n",
    "    :return (dict): for each stat.X.out dictionary is created (keys and corresponding list of values). For our example\n",
    "    there is only one key that we are interested in (key 3 - 'GaussianEMRestraint_None_CCC')\n",
    "    \"\"\"\n",
    "\n",
    "    # Open the file and reads all the frames into \"content\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.readlines()\n",
    "\n",
    "    # Initializes an empty list 'data' to store dictionaries obtained from the file.\n",
    "    data = []\n",
    "    # To skip first dictionary\n",
    "    skip_first_dict = True\n",
    "    dict_lines = []\n",
    "\n",
    "    # The purpose of this part of the code is to skip the first dictionary in the file\n",
    "    for line in content:\n",
    "        if skip_first_dict:\n",
    "            dict_lines.append(line.strip())\n",
    "            # Check if we've reached the end of the first dictionary\n",
    "            if dict_lines.count('{') == dict_lines.count('}'):\n",
    "                # When we reach the end of first dictionary, skipping is turned off\n",
    "                skip_first_dict = False\n",
    "            continue\n",
    "\n",
    "        # Tries to parse the stripped line into a dictionary using ast.literal_eval\n",
    "        # If successful, appends the dictionary to data. If it fails, it continues to the next line.\n",
    "        try:\n",
    "            dictionary = ast.literal_eval(line.strip())\n",
    "            data.append(dictionary)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # New dictionary where the extracted data from each dictionary in the 'data' list should be stored\n",
    "    extracted_data = {key: [] for key in keys}\n",
    "\n",
    "    # Extracting values from the 'data' list and combining them with corresponding keys\n",
    "    for dictionary in data:\n",
    "        for key in keys:\n",
    "            extracted_data[key].append(dictionary.get(key, ''))\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "def extracting_stat_files(state_dict, runs_nr, replica_nr, replica_output_name, keys_to_extract, decimals_nr, custom_base_path = None):\n",
    "    \"\"\"\n",
    "    This function extracts and collects all the relevant scores from all the runs. To understand this function, it is\n",
    "    important to understand stat files (stat.X.out) created with pmi snapshot.py and IMP.pmi.macros.ReplicaExchange\n",
    "    command in snapshot.py.\n",
    "\n",
    "    :param state_dict (dict): dictionary that defines the spatiotemporal model.\n",
    "           The keys are strings that correspond to each time point in the\n",
    "           stepwise temporal process. Keys should be ordered according to the\n",
    "           steps in the spatiotemporal process. The values are integers that\n",
    "           correspond to the number of possible states at that timepoint.\n",
    "    :param runs_nr (int): number of runs generated with start_sim.py for each snapshot{state}_{time}\n",
    "    :param replica_nr (int): number of processor required for ReplicaExchange set in Job.sh\n",
    "    :param replica_output_name (str): name of output file set in IMP.pmi.macros.ReplicaExchange command\n",
    "    in pmi snapshot.py\n",
    "    :param keys_to_extract (list): List of strings which represents keys in stat file dictionary. For example: number 3\n",
    "    corresponds to 'GaussianEMRestraint_None_CCC' in first dictionary of our model.\n",
    "    Therefore, in each further dictionary in stat.X.out values related to key '3' corresponds to\n",
    "    GaussianEMRestraint_None_CCC for certain frame.\n",
    "    :param decimals_nr (int): Number of decimals that should be extracted from dictionaries of stat.X.out.\n",
    "    For example: GaussianEMRestraint_None_CCC values have 16 decimals\n",
    "    :param custom_base_path (optional - str): Custom path to the directory where snapshot{state}_{time} created with\n",
    "    start_sim.py are\n",
    "    :return: {state}_{time}_stat.txt is created in each snapshot{state}_{time} directory\n",
    "    \"\"\"\n",
    "    if custom_base_path:\n",
    "        base_path = custom_base_path\n",
    "    else:\n",
    "        base_path = \"../Snapshots_Modeling\"\n",
    "    for time in state_dict.keys():\n",
    "        for state in range(1, state_dict[time] + 1):\n",
    "            dir_name = f\"snapshot{state}_{time}\"\n",
    "            dir_path = os.path.join(base_path, dir_name)\n",
    "\n",
    "            if not os.path.exists(dir_path):\n",
    "                print(f\"Directory {dir_path} does not exist. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Create the output file once for each directory\n",
    "            output_file_path = os.path.join(dir_path, f\"{state}_{time}_stat.txt\")\n",
    "            with open(output_file_path, 'w') as out_file:\n",
    "                out_file.write(\"\\t\".join(map(str, keys_to_extract)) + \"\\n\")\n",
    "\n",
    "                for run in range(1, runs_nr + 1):\n",
    "                    run_dir = os.path.join(dir_path, f\"run{run}\")\n",
    "\n",
    "                    if not os.path.exists(run_dir):\n",
    "                        print(f\"Run directory {run_dir} does not exist. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    combined_data = {key: [] for key in keys_to_extract}\n",
    "\n",
    "                    for stat_file_num in range(replica_nr):\n",
    "                        stat_file_path = os.path.join(run_dir, replica_output_name, f\"stat.{stat_file_num}.out\")\n",
    "\n",
    "                        if not os.path.exists(stat_file_path):\n",
    "                            print(f\"Stat file {stat_file_path} does not exist. Skipping.\")\n",
    "                            continue\n",
    "\n",
    "                        extracted_data = extract_values_from_file(stat_file_path, keys_to_extract)\n",
    "\n",
    "                        for key in keys_to_extract:\n",
    "                            combined_data[key].extend(extracted_data[key])\n",
    "\n",
    "                    # Transpose and write the data\n",
    "                    rows = zip(*[combined_data[key] for key in keys_to_extract])\n",
    "                    for row in rows:\n",
    "                        out_file.write(\n",
    "                            \"\\t\".join(map(lambda x: f\"{x:.{decimals_nr}f}\" if isinstance(x, float) else str(x),\n",
    "                                          row)) + \"\\n\")\n",
    "\n",
    "            print(f\"Stat file {state}_{time}_stat.txt created.\")\n",
    "\n",
    "def general_rule_calculation(state_dict, general_rule_column, custom_general_rule_file = None, custom_base_path = None):\n",
    "    \"\"\"\n",
    "    From each {state}_{time}_stat.txt created with extracting_stat_files function median value of desired column is\n",
    "    calculated and results are gathered in general_rule_file.\n",
    "\n",
    "    :param state_dict (dict): dictionary that defines the spatiotemporal model.\n",
    "           The keys are strings that correspond to each time point in the\n",
    "           stepwise temporal process. Keys should be ordered according to the\n",
    "           steps in the spatiotemporal process. The values are integers that\n",
    "           correspond to the number of possible states at that timepoint.\n",
    "    :param general_rule_column (int): Column from {state}_{time}_stat.txt on which general rule should be applied.\n",
    "    For example: column '3' corresponds to 'GaussianEMRestraint_None_CCC' values in our example.\n",
    "    :param custom_general_rule_file (optional - str): Custom name of output file created with this function\n",
    "    :param custom_base_path (optional - str): Custom path to the directory where snapshot{state}_{time} created with\n",
    "    start_sim.py are\n",
    "    :return: general_rule_file .txt saved in the directory where this code is\n",
    "\n",
    "    NOTE: if desired, name 'Median_Value_ccEM' can be manually changed in the 'columns' list\n",
    "    \"\"\"\n",
    "\n",
    "    # optional parameters\n",
    "    if custom_base_path:\n",
    "        base_path = custom_base_path\n",
    "    else:\n",
    "        base_path = \"../Snapshots_Modeling\"\n",
    "\n",
    "    if custom_general_rule_file:\n",
    "        general_rule_file = custom_general_rule_file\n",
    "    else:\n",
    "        general_rule_file = 'general_rule.txt'\n",
    "\n",
    "    output_data = [] # Here all the data is temporarily saved\n",
    "    for time in state_dict.keys():\n",
    "        for state in range(1, state_dict[time] + 1):\n",
    "            dir_name = f\"snapshot{state}_{time}\" # dir name is the first column in the general_rule_file .txt\n",
    "            file_path = os.path.join(base_path, dir_name, f'{state}_{time}_stat.txt')\n",
    "\n",
    "            # From {state}_{time}_stat.txt different following things are extracted\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path, sep='\\s+')\n",
    "\n",
    "                num_rows = df.shape[0]  # Number of rows, meaning number of generated rmfs for certain snapshot\n",
    "                median_value = df[f'{general_rule_column}'].median() # calculation of general rule\n",
    "                output_data.append([dir_name, num_rows, median_value])\n",
    "            print(f'Extracting data for {dir_name}')\n",
    "\n",
    "    output_df = pd.DataFrame(output_data,\n",
    "                             columns=['Directory', 'Number_of_Generated_Frames', 'Median_Value_ccEM']) # header\n",
    "    output_df.to_csv(general_rule_file, index=False, sep='\\t') # create an output file: general_rule_file\n",
    "\n",
    "def general_rule_filter_independent_samples(state_dict, _main_dir, custom_general_rule_file = None, custom_base_path = None):\n",
    "    \"\"\"\n",
    "    This function has two main roles: (i) filtering frames based on general rule and (ii) creating two independent\n",
    "    samples for each snapshot from filtered frames. Both steps can be achieved by submitting the imp_sampcon select_good\n",
    "    command in each snapshot{state}_{time} directory. More about imp_sampcon select_good can be found here:\n",
    "    https://integrativemodeling.org/tutorials/actin/analysis.html\n",
    "    (i) median value calculated in general_rule_file .txt is set as a lower filtering threshold. Filtered frames (rmfs)\n",
    "    are then saved in  each snapshot{state}_{time} directory, where new directory 'good_scoring_models' is created.\n",
    "    (ii) The flag -e in imp_sampcon select_good command results in creating 'good_scoring_models', a folder with the\n",
    "    models that passed through our filtering criteria. Data from this directory is later used for exahust function as\n",
    "    well as in the 'Trajectories_Assessment' step.\n",
    "\n",
    "    :param state_dict (dict): dictionary that defines the spatiotemporal model.\n",
    "           The keys are strings that correspond to each time point in the\n",
    "           stepwise temporal process. Keys should be ordered according to the\n",
    "           steps in the spatiotemporal process. The values are integers that\n",
    "           correspond to the number of possible states at that timepoint.\n",
    "    :param _main_dir (str): directory where this code is\n",
    "    :param custom_general_rule_file (optional - str): Custom name of output file created with this function\n",
    "    :param custom_base_path (optional - str): Custom path to the directory where snapshot{state}_{time} created with\n",
    "    start_sim.py are\n",
    "    \"\"\"\n",
    "    # optional parameters\n",
    "    if custom_base_path:\n",
    "        base_path = custom_base_path\n",
    "    else:\n",
    "        base_path = \"../Snapshots_Modeling\"\n",
    "\n",
    "    if custom_general_rule_file:\n",
    "        general_rule_file = custom_general_rule_file\n",
    "    else:\n",
    "        general_rule_file = 'general_rule.txt'\n",
    "\n",
    "    # Read the text file and create temporary dictionary directory (keys) vs median values (values)\n",
    "    median_values = {}\n",
    "    with open(general_rule_file, 'r') as file:\n",
    "        next(file)  # Skip the header line\n",
    "        for line in file: # in each line we are only interested in the 1st and 3rd element\n",
    "            parts = line.split()  # Split the line into a list of strings\n",
    "            directory = parts[0]  # First part is the directory name\n",
    "            general_rule = parts[2]  # Third part is the median value\n",
    "            median_values[directory] = general_rule  # Store in dictionary\n",
    "\n",
    "    # Execute command in each directory with the corresponding median value\n",
    "    for time in state_dict.keys():\n",
    "        for state in range(1, state_dict[time] + 1):\n",
    "            dir_name = f\"snapshot{state}_{time}\"\n",
    "            if dir_name in median_values:\n",
    "                median_value = median_values[dir_name]\n",
    "                command = f\"imp_sampcon select_good -rd . -rp run -sl 'GaussianEMRestraint_None_CCC' -pl ConnectivityRestraint_Score \\ExcludedVolumeSphere_Score \\GaussianEMRestraint_None \\Total_Score -alt {median_value} -aut 1.0 -e\"\n",
    "\n",
    "                print(f\"Executing in {dir_name}: {command}\")\n",
    "                os.chdir(os.path.join(base_path, dir_name))  # Navigate to each snapshot directory where the command (-rd) is executed\n",
    "                subprocess.check_call(command, shell=True)  # Execute the command\n",
    "                os.chdir(_main_dir)  # Change back to the universal directory to restart the loop\n",
    "\n",
    "# state_dict - universal parameter\n",
    "state_dict = {'2min': 1}\n",
    "# main directory\n",
    "os.chdir(main_dir)\n",
    "assessment_folder=os.path.join(main_dir, 'Snapshots_Assessment')\n",
    "if not os.path.exists(assessment_folder):\n",
    "    os.system('mkdir '+assessment_folder)\n",
    "os.chdir(assessment_folder)\n",
    "\n",
    "# 1 calling extracting_stat_files function and related parameters\n",
    "keys_to_extract = [3]\n",
    "runs_nr = 2\n",
    "replica_nr = 1\n",
    "replica_output_name = 'output'\n",
    "decimals_nr = 16\n",
    "\n",
    "extracting_stat_files(state_dict, runs_nr, replica_nr, replica_output_name, keys_to_extract, decimals_nr, custom_base_path = main_dir)\n",
    "print(\"extracting_stat_files is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "# 2 calling general_rule_calculation and related parameters\n",
    "general_rule_column = '3'\n",
    "\n",
    "general_rule_calculation(state_dict, general_rule_column, custom_base_path = main_dir)\n",
    "\n",
    "print(\"general_rule_calculation is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "with open('general_rule.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)\n",
    "\n",
    "\n",
    "# 3 calling general_rule_filter_independent_samples\n",
    "general_rule_filter_independent_samples(state_dict, assessment_folder, custom_base_path = main_dir)\n",
    "print(\"general_rule_filter_independent_samples is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b6702c",
   "metadata": {
    "id": "35b6702c"
   },
   "source": [
    "## Plotting data, clustering models, and determining sampling precision<a id=\"notebook_snapshot_sampling_precision\"></a>\n",
    "\n",
    "Next, scores can be plotted for analysis. Here, we wrote the `create_histograms` function to run `imp_sampcon plot_score` so that it plots distributions for various scores of interest. Each of these plots are saved to `histograms{state}_{time}/{score}.png`, where score is an object listed in the `score_list`. These plots are useful for debugging the modeling protocol, and should appear roughly Gaussian. Here, we note that the plot does not appear roughly Gaussian, indicating that sampling is likely not converged.\n",
    "\n",
    "We then check the number of models in each sampling run though our function, `count_rows_and_generate_report`, which writes the `independent_samples_stat.txt` file. Empirically, we have found that ensuring the overall number of models in each independent sample after filtering is roughly equal serves a good first check on sampling convergence. We print the results of this test, which will likely show that one sampling run has more filtered samples (or \"rows\") than the other sampling run.\n",
    "\n",
    "Next, we write the density range dictionaries, which are output as `{state}_{time}_density_ranges.txt`. These dictionaries label each protein in each snapshot model, which will be passed into `imp_sampcon` to calculate the localization density of each protein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813c9d2",
   "metadata": {
    "id": "b813c9d2"
   },
   "outputs": [],
   "source": [
    "def create_histograms(state_dict, _main_dir, score_list, custom_base_path = None):\n",
    "    \"\"\"\n",
    "    This function creates different \"score histograms\" to visualize score distributions after filtering based on\n",
    "    general rule. Histograms are created in each newly created histograms{state}_{time} directory by submitting\n",
    "    imp_sampcon plot_score command there. More about imp_sampcon plot_score can be found here:\n",
    "    https://integrativemodeling.org/tutorials/actin/analysis.html\n",
    "\n",
    "    :param state_dict (dict): dictionary that defines the spatiotemporal model.\n",
    "           The keys are strings that correspond to each time point in the\n",
    "           stepwise temporal process. Keys should be ordered according to the\n",
    "           steps in the spatiotemporal process. The values are integers that\n",
    "           correspond to the number of possible states at that timepoint.\n",
    "    :param _main_dir (str): directory where this code is\n",
    "    :param score_list (list of str): columns in 'model_ids_scores.txt' for which histograms should be created.\n",
    "    For example: for this tutorial we created histograms for all scores defined in the imp_sampcon select_good\n",
    "    (inside the general_rule_filter_independent_samples function)\n",
    "    :param custom_base_path (optional - str): Custom path to the directory where snapshot{state}_{time} created with\n",
    "    start_sim.py are\n",
    "    \"\"\"\n",
    "\n",
    "    # optional parameter\n",
    "    if custom_base_path:\n",
    "        base_path = custom_base_path\n",
    "    else:\n",
    "        base_path = \"../Snapshots_Modeling\"\n",
    "\n",
    "    for time in state_dict.keys():\n",
    "        for state in range(1, state_dict[time] + 1):\n",
    "            # For each snapshot separate directory should be created to avoid overlapping of the plots, also this makes eveything more clear\n",
    "            dir_name = f\"histograms{state}_{time}\"\n",
    "            # Construct the full path to plots{state}_{time} directory where command is executed\n",
    "            full_path = os.path.join(_main_dir, dir_name)\n",
    "            os.makedirs(full_path, exist_ok=True)\n",
    "            os.chdir(full_path)\n",
    "            for score in score_list:\n",
    "                # command is executed in newly created histogram{state}_{time} directory, therefore additional '../' should be added\n",
    "                command = f\"imp_sampcon plot_score  {base_path}/snapshot{state}_{time}/good_scoring_models/model_ids_scores.txt \\\n",
    "      {score}\"\n",
    "                os.system(command) # Execute the command\n",
    "                os.system('imp_sampcon plot_score _h') # Execute the command\n",
    "                print(f\"Executing command: {command}\")\n",
    "            os.chdir(_main_dir) # Change back to the universal directory to restart the loop after all the histograms are created\n",
    "\n",
    "def count_rows_and_generate_report(state_dict, custom_output_file_path = None, custom_base_path = None):\n",
    "    \"\"\"\n",
    "    This function counts number of rmfs in each of two independent samples to check if distribution is similar. Such\n",
    "    brief check can predict if we can expect some problems (especially with K-S test) in achieving sampling\n",
    "    exhaustiveness in the next step (exhaust function). Writes a file to output_file_path.txt with the number of\n",
    "    samples that passed filtering in each independent set of sampling runs.\n",
    "\n",
    "    :param state_dict (dict): dictionary that defines the spatiotemporal model.\n",
    "           The keys are strings that correspond to each time point in the\n",
    "           stepwise temporal process. Keys should be ordered according to the\n",
    "           steps in the spatiotemporal process. The values are integers that\n",
    "           correspond to the number of possible states at that timepoint.\n",
    "    :param custom_output_file_path: Custom name of output_file_path .txt\n",
    "    :param custom_base_path (optional - str): Custom path to the directory where snapshot{state}_{time} created with\n",
    "    start_sim.py are\n",
    "    \"\"\"\n",
    "\n",
    "    # optional parameters\n",
    "    if custom_base_path:\n",
    "        base_path = custom_base_path\n",
    "    else:\n",
    "        base_path = \"../Snapshots_Modeling\"\n",
    "\n",
    "    if custom_output_file_path:\n",
    "        output_file_path = custom_output_file_path\n",
    "    else:\n",
    "        output_file_path = 'independent_samples_stat.txt'\n",
    "\n",
    "    output_data = []\n",
    "    for time in state_dict.keys():\n",
    "        for state in range(1, state_dict[time] + 1):\n",
    "            snapshot_dir = f'snapshot{state}_{time}'\n",
    "            good_scoring_models_dir = os.path.join(base_path, snapshot_dir, 'good_scoring_models')\n",
    "\n",
    "            # Initialize row counts\n",
    "            rows_A = 0\n",
    "            rows_B = 0\n",
    "\n",
    "            # Count rows in scoresA.txt\n",
    "            scoresA_path = os.path.join(good_scoring_models_dir, 'scoresA.txt')\n",
    "            if os.path.exists(scoresA_path):\n",
    "                with open(scoresA_path, 'r') as file:\n",
    "                    rows_A = sum(1 for line in file)\n",
    "\n",
    "            # Count rows in scoresB.txt\n",
    "            scoresB_path = os.path.join(good_scoring_models_dir, 'scoresB.txt')\n",
    "            if os.path.exists(scoresB_path):\n",
    "                with open(scoresB_path, 'r') as file:\n",
    "                    rows_B = sum(1 for line in file)\n",
    "\n",
    "            # Calculate total rows\n",
    "            total_rows = rows_A + rows_B\n",
    "\n",
    "            # Append data to the output list\n",
    "            output_data.append([snapshot_dir, rows_A, rows_B, total_rows])\n",
    "\n",
    "    # Write the output data to a text file\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        # Write header\n",
    "        output_file.write('Directory\\tRows in scoresA.txt\\tRows in scoresB.txt\\tTotal Rows\\n')\n",
    "\n",
    "        # Write data rows\n",
    "        for data in output_data:\n",
    "            output_file.write('\\t'.join(map(str, data)) + '\\n')\n",
    "\n",
    "    print(f\"Output written to {output_file_path}\") # print to check that function is completed\n",
    "\n",
    "def extract_molecule_names(file_path):\n",
    "    '''\n",
    "    This function extracts molecule names from corresponding topology files. Used by create_density_dictionary_files.\n",
    "\n",
    "    :param file_path: Path to corresponding topology files.\n",
    "    :return (set): Set of all extracted molecule names.\n",
    "    '''\n",
    "\n",
    "    molecule_names = set()  # set for uniqueness (in topology file there are different (super) rigid bodies\n",
    "    with open(file_path, 'r') as file:\n",
    "        next(file)  # skip the header\n",
    "        for line in file:\n",
    "            if not line.strip():  # skip the empty line\n",
    "                continue\n",
    "            parts = line.split('|')  # split lines based on the |\n",
    "            if len(parts) > 1:  # just to be sure we are splitting right lines\n",
    "                molecule_name = parts[1].strip()  # Read the first column (index 1 after splitting)\n",
    "                molecule_names.add(molecule_name)  # add molecules to the set\n",
    "    print(f'Extracted molecule names from {file_path}: {molecule_names}')\n",
    "    return molecule_names\n",
    "\n",
    "def create_density_dict(molecule_names):\n",
    "    '''\n",
    "    This function creates the density dictionaries ({state}_{time}_density_ranges.txt) based on extracted\n",
    "    molecule names. Used by create_density_dictionary_files.\n",
    "\n",
    "    :param molecule_names (set): Set of all extracted molecule names created with extract_molecule_names function\n",
    "    :return (dict): Dictionary that will be used later for the {state}_{time}_density_ranges.txt.\n",
    "    '''\n",
    "\n",
    "    if not molecule_names:\n",
    "        print('No molecule names found to create the density dictionary.')\n",
    "        return {}\n",
    "    # MOST IMPORTANT THING: to create dictionary from molecule_names set. Key and value are the same\n",
    "    density_dict = {name: [name] for name in molecule_names}\n",
    "    return density_dict\n",
    "\n",
    "def write_density_file(output_path, density_dict):\n",
    "    '''\n",
    "    This function converts dictionaries to {state}_{time}_density_ranges.txt that are used in exahust function. Used by\n",
    "    create_density_dictionary_files.\n",
    "\n",
    "    :param output_path (str): Path where {state}_{time}_density_ranges.txt should be saved. For exhaust function to\n",
    "    work properly, all {state}_{time}_density_ranges.txt should be saved in this working directory\n",
    "    :param density_dict (dict): Density dictionary created with create_density_dict function.\n",
    "    '''\n",
    "    with open(output_path, 'w') as file:\n",
    "        file.write(f'density_custom_ranges={density_dict}')\n",
    "\n",
    "def create_density_dictionary_files(state_dict, _main_dir, custom_base_path = None):\n",
    "    '''\n",
    "    This function creates the density dictionaries ({state}_{time}_density_ranges.txt) based on the corresponding\n",
    "    topology file. These {state}_{time}_density_ranges.txt are then used later in exhaust functon. This allows to\n",
    "    visualize/present .mrc density map for each protein in certain snapshot separately. Density dictionaries are\n",
    "    created in three steps:\n",
    "    -extracting molecule names from corresponding topology files\n",
    "    -creating dictionaries for density maps\n",
    "    -converting dictionaries to {state}_{time}_density_ranges.txt\n",
    "\n",
    "    :param state_dict (dict): dictionary that defines the spatiotemporal model.\n",
    "           The keys are strings that correspond to each time point in the\n",
    "           stepwise temporal process. Keys should be ordered according to the\n",
    "           steps in the spatiotemporal process. The values are integers that\n",
    "           correspond to the number of possible states at that timepoint.\n",
    "    :param _main_dir (str): current working directory\n",
    "    :param custom_base_path (str): Custom path to the directory to the Snapshot_Modeling directory\n",
    "    (where {state}_{time}_topol.txt are)\n",
    "    '''\n",
    "\n",
    "    # optional parameter\n",
    "    if custom_base_path:\n",
    "        base_path = custom_base_path\n",
    "    else:\n",
    "        base_path = \"../../Heterogeneity/Heterogeneity_Modeling\"\n",
    "\n",
    "    for time in state_dict.keys():\n",
    "        for state in range(1, state_dict[time] + 1):\n",
    "            input_file = os.path.join(base_path,\n",
    "                                      f'{state}_{time}_topol.txt')  # path needs to be changed!! Maybe also some parameters\n",
    "            output_file = os.path.join(_main_dir, f'{state}_{time}_density_ranges.txt')\n",
    "\n",
    "            # Using time-state loop we call all three function one after another (only in case we can access topology files)\n",
    "            if os.path.exists(input_file):\n",
    "                molecule_names = extract_molecule_names(input_file)\n",
    "                density_dict = create_density_dict(molecule_names)\n",
    "                write_density_file(output_file, density_dict)\n",
    "                print(f'Created density file: {output_file}')\n",
    "            else:\n",
    "                print(f'Input file does not exist: {input_file}')\n",
    "\n",
    "\n",
    "# 4 calling create_histograms and related parameters\n",
    "score_list = [\n",
    "        'Total_Score',\n",
    "        'ConnectivityRestraint_Score',\n",
    "        'ExcludedVolumeSphere_Score',\n",
    "        'GaussianEMRestraint_None',\n",
    "        'GaussianEMRestraint_None_CCC'\n",
    "] # list of histograms we want to create in each histograms{state}_{time} directory\n",
    "\n",
    "create_histograms(state_dict, assessment_folder, score_list, custom_base_path = main_dir)\n",
    "print(\"create_histograms is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "# Display histogram of total score\n",
    "img = Image.open(f'histograms{state}_{time}/Total_Score.png')\n",
    "base_width=300\n",
    "wpercent = (base_width / float(img.size[0]))\n",
    "hsize = int((float(img.size[1]) * float(wpercent)))\n",
    "img = img.resize((base_width, hsize))\n",
    "display(img)\n",
    "\n",
    "\n",
    "# 5 calling count_rows_and_generate_report\n",
    "count_rows_and_generate_report(state_dict, custom_base_path = main_dir)\n",
    "print(\"count_rows_and_generate_report is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "with open('independent_samples_stat.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)\n",
    "\n",
    "# 6 calling create_density_dictionary:\n",
    "create_density_dictionary_files(state_dict, assessment_folder, custom_base_path = main_dir)\n",
    "print(\"create_density_dictionary is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb8c2f",
   "metadata": {
    "id": "0cbb8c2f"
   },
   "source": [
    "\n",
    "Next, we would run `imp_sampcon exhaust` on each snapshot model. This code performs checks on the exhaustiveness of the sampling. Specifically it analyzes the convergence of the model score, whether the two model sets were drawn from the same distribution, and whether each structural cluster includes models from each sample proportionally to its size. The output for each snapshot model is written out to the `exhaust_{state}_{time}` folder.\n",
    "\n",
    "Further structural analysis can be calculated by using the `cluster.*` files. The `cluster.*.{sample}.txt` files contain the model number for the models in that cluster, where `{sample}` indicates which round of sampling the models came from. The `cluster.*` folder contains an RMF for centroid model of that cluster, along with the localization densities for each protein. The localization densities of each protein from each independent sampling can be compared to ensure independent samplings produce the same results.\n",
    "\n",
    "Ideally, each of these plots should be checked for each snapshot model. As a way to summarize the output of these checks, we can gather the results of the KS test and the sampling precision test for all snapshot models. This is done by running `extract_exhaust_data` and `save_exhaust_data_as_png`, which write `KS_sampling_precision_output.txt` and `KS_sampling_precision_output.png`, respectively.\n",
    "\n",
    "```python\n",
    "# 7 calling exhaust\n",
    "exhaust(state_dict, main_dir)\n",
    "print(\"exhaust is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "# 8 calling extract_exhaust_data\n",
    "extract_exhaust_data(state_dict)\n",
    "print(\"extract_exhaust_data is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "# 9 calling save_exhaust_data_as_png\n",
    "save_exhaust_data_as_png()\n",
    "print(\"save_exhaust_data_as_png is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "```\n",
    "\n",
    "For simplicity, we will only perform one of several checks perfromed by `imp_sampcon exhaust`. Namely, we will run a non-parametric Kolmogorov–Smirnov (KS) two-sample test on two independently sampled sets of scores, to determine whether the scores were drawn from the same parent distribution, which will be passed if the difference is both statistically insignificant (p>0.05) and small in magnitude (D<0.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d23a0",
   "metadata": {
    "id": "524d23a0"
   },
   "outputs": [],
   "source": [
    "def KS_test(state_dict, _main_dir, custom_base_path = None):\n",
    "    \"\"\"\n",
    "    Based on all filtered frames for each snapshot this function runs a KS_test to check for sampling convergence.\n",
    "    This is one of several functions performed by imp_sampcon exhaust. More about imp_sampcon exhaust can be found here:\n",
    "    https://integrativemodeling.org/tutorials/actin/analysis.html\n",
    "\n",
    "    :param state_dict (dict): dictionary that defines the spatiotemporal model.\n",
    "           The keys are strings that correspond to each time point in the\n",
    "           stepwise temporal process. Keys should be ordered according to the\n",
    "           steps in the spatiotemporal process. The values are integers that\n",
    "           correspond to the number of possible states at that timepoint.\n",
    "    :param _main_dir (str): directory where this code is\n",
    "    :param custom_base_path (optional - str): Custom path to the directory where snapshot{state}_{time} created with\n",
    "    start_sim.py are\n",
    "    \"\"\"\n",
    "\n",
    "    # optional parameter\n",
    "    if custom_base_path:\n",
    "        base_path = custom_base_path\n",
    "    else:\n",
    "        base_path = \"../Snapshots_Modeling\"\n",
    "\n",
    "    for time in state_dict.keys():\n",
    "        for state in range(1, state_dict[time] + 1):\n",
    "            score_fileA = os.path.join(base_path, f'snapshot{state}_{time}', 'good_scoring_models', 'scoresA.txt')\n",
    "            score_fileB = os.path.join(base_path, f'snapshot{state}_{time}', 'good_scoring_models', 'scoresB.txt')\n",
    "            scoresA=np.loadtxt(score_fileA)\n",
    "            scoresB=np.loadtxt(score_fileB)\n",
    "            # evaluate the difference between the 2 sets of scores\n",
    "            result=stats.kstest(scoresA,scoresB)\n",
    "            print(f'KS test p-value: {result.pvalue}')\n",
    "            print(f'KS test statistic (D): {result.statistic}')\n",
    "            # plot the a histogram of each score -------------------------------\n",
    "            # Set colors\n",
    "            color1 = [0, 0.4470, 0.7410]\n",
    "            color2 = [0.8500, 0.3250, 0.0980]\n",
    "            # Create the histogram\n",
    "            nbins=20\n",
    "            histA, bin_edges = np.histogram(scoresA,bins=nbins, range=( np.min(np.hstack((scoresA, scoresB))), np.max(np.hstack((scoresA, scoresB))) ) )\n",
    "            histB, bin_edges = np.histogram(scoresB,bins=nbins, range=( np.min(np.hstack((scoresA, scoresB))), np.max(np.hstack((scoresA, scoresB))) ) )\n",
    "            plotting_edges=bin_edges[0:nbins]\n",
    "            plotting_edges+=(bin_edges[1]-bin_edges[0])/2\n",
    "            # Plot histogram\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "            ax.plot(plotting_edges, histA, markerfacecolor=color1, markersize=8, linewidth=3, color=color1)\n",
    "            ax.plot(plotting_edges, histB, markerfacecolor=color2, markersize=8, linewidth=3, color=color2)\n",
    "            ax.set_xlabel('Score')\n",
    "            ax.set_ylabel('Number of models')\n",
    "            ax.legend([' Sample 1', ' Sample 2'], loc='best')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# KS test - one of four checks performed by imp_sampcon exhaust\n",
    "KS_test(state_dict, assessment_folder, custom_base_path = main_dir)\n",
    "print(\"KS_test is DONE\")\n",
    "print(\"\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2546f-7c6f-4146-8c9b-ee143fcead6e",
   "metadata": {
    "id": "6fc2546f-7c6f-4146-8c9b-ee143fcead6e"
   },
   "source": [
    "The above plot and results of the KS test above show that the 2 score distributions are statistically different (p<0.05 and/or D>0.3), indicating that more sampling is necessary. As a comparison, we show all 4 sampling plots for the complete sampling of the same snapshot model, 1_2min. (a) Tests the convergence of the lowest scoring model (`snapshot_{state}_{time}.Top_Score_Conv.pdf`). Error bars represent standard deviations of the best scores, estimated by selecting 150 different subsets of models. The light-blue line helps compare the highest average top score to the top score from other subsets of models. (b) Tests that the scores of two independently sampled models come from the same distribution (`snapshot_{state}_{time}.Score_Dist.pdf`). The difference between the two distributions, as measured by the KS test statistic (D) and KS test p-value (p) indicates that the difference is both statistically insignificant (p>0.05) and small in magnitude (D<0.3). (c) Determines the structural precision of a snapshot model (`snapshot_{state}_{time}.ChiSquare.pdf`). RMSD clustering is performed at 1 Å intervals until the clustered population (% clustered) is greater than 80%, and either the χ<sup>2</sup> p-value is greater than 0.05 or Cramer’s V is less than 0.1. The sampling precision is indicated by the dashed black line. (d) Populations from sample 1 and sample 2 are shown for each cluster (`snapshot_{state}_{time}.Cluster_Population.pdf`).\n",
    "\n",
    "<img src=\"https://github.com/salilab/imp_spatiotemporal_tutorial/blob/main/Jupyter/images/Snapshot_Exhaust.png?raw=1\" width=\"600px\" />\n",
    "\n",
    "This code also writes a table that includes the KS two sample test statistic (D), the KS test p-value, and the sampling precision for each snapshot model, which is replotted below.\n",
    "\n",
    "<img src=\"https://github.com/salilab/imp_spatiotemporal_tutorial/blob/main/Jupyter/images/Snapshot_sampling.png?raw=1\" width=\"300px\" />\n",
    "\n",
    "\n",
    "## Visualizing models<a id=\"notebook_snapshot_visualization\"></a>\n",
    "\n",
    "The resulting RMF files and localization densities from this analysis can be viewed in [UCSF Chimera](https://www.rbvi.ucsf.edu/chimera/) (version>=1.13) or [UCSF ChimeraX](https://www.cgl.ucsf.edu/chimerax/).\n",
    "\n",
    "Here, we plotted each centroid model (A - blue, B - orange, and C - purple) from the most populated cluster for each snapshot model and compared that model to the experimental EM profile (gray).\n",
    "\n",
    "<img src=\"https://github.com/salilab/imp_spatiotemporal_tutorial/blob/main/Jupyter/images/static_snapshots_noCC.png?raw=1\" width=\"300px\" />\n",
    "\n",
    "Finally, now that snapshot models were assessed, we can perform modeling of a trajectory.\n",
    "\n",
    "Modeling of a Trajectory<a id=\"notebook_trajectories\"></a>\n",
    "====================================\n",
    "\n",
    "Here, we describe the final modeling problem in our composite workflow, how to build a trajectory model using IMP.\n",
    "\n",
    "# Trajectory modeling step 1: gathering of information<a id=\"notebook_trajectories1\"></a>\n",
    "\n",
    "We begin trajectory modeling with the first step of integrative modeling, gathering information. Trajectory modeling utilizes dynamic information about the bimolecular process. In this case, we utilize snapshot models, physical theories, and synthetically generated small-angle X-ray scattering (SAXS) profiles.\n",
    "\n",
    "<img src=\"https://github.com/salilab/imp_spatiotemporal_tutorial/blob/main/Jupyter/images/Input_trajectories.png?raw=1\" width=\"600px\" />\n",
    "\n",
    "Snapshot models inform transitions between sampled time points, and their scores inform trajectory scores. Physical theories of macromolecular dynamics inform transitions between sampled time points. SAXS data informs the size and shape of the assembling complex and is left for validation.\n",
    "\n",
    "# Trajectory modeling step 2: representation, scoring function, and search process<a id=\"notebook_trajectories2\"></a>\n",
    "\n",
    "Trajectory modeling connects alternative snapshot models at adjacent time points, followed by scoring the trajectories based on their fit to the input information, as described in full [here](https://www.biorxiv.org/content/10.1101/2024.08.06.606842v1.abstract).\n",
    "\n",
    "## Background behind integrative spatiotemporal modeling<a id=\"autotoc1v42\"></a>\n",
    "### Representing the model<a id=\"notebook_trajectory_representation\"></a>\n",
    "\n",
    "We choose to represent dynamic processes as a trajectory of snapshot models, with one snapshot model at each time point. In this case, we computed snapshot models at 3 time points (0, 1, and 2 minutes), so a single trajectory will consist of 3 snapshot models, one at each 0, 1, and 2 minutes. The modeling procedure described here will produce a set of scored trajectories, which can be displayed as a directed acyclic graph, where nodes in the graph represent the snapshot model and edges represent connections between snapshot models at neighboring time points.\n",
    "\n",
    "### Scoring the model<a id=\"notebook_trajectory_scoring\"></a>\n",
    "\n",
    "To score trajectories, we incorporate both the scores of individual snapshot models, as well as the scores of transitions between them. Under the assumption that the process is Markovian (*i.e.* memoryless), the weight of a trajectory takes the form:\n",
    "\n",
    "$$\n",
    "W(\\chi) \\propto   \\displaystyle\\prod^{T}_{t=0} P( X_{t} | D_{t}) \\cdot \\displaystyle\\prod^{T-1}_{t=0} W(X_{t+1} | X_{t},D_{t,t+1}),\n",
    "$$\n",
    "\n",
    "where $t$ indexes times from 0 until the final modeled snapshot ($T$); $P(X_{t} | D_{t})$ is the snapshot model score; and $W(X_{t+1} | X_{t},D_{t,t+1})$ is the transition score. Trajectory weights ($W(\\chi)$) are normalized so that the sum of all trajectory weights is 1.0. Transition scores are currently based on a simple metric that either allows or disallows a transition. Transitions are only allowed if all proteins in the first snapshot model are included in the second snapshot model. In the future, we hope to include more detailed transition scoring terms, which may take into account experimental information or physical models of macromolecular dynamics.\n",
    "\n",
    "### Searching for good scoring models<a id=\"notebook_trajectory_searching\"></a>\n",
    "\n",
    "Trajectories are constructed by enumerating all connections between adjacent snapshot models and scoring these trajectories according to the equation above. This procedure results in a set of weighted trajectories.\n",
    "\n",
    "## Computing trajectory models<a id=\"autotoc1v43\"></a>\n",
    "To compute trajectories, we first copy all necessary files to a new directory, `data`. These files are (i) `{state}_{time}.config` files, which include the subcomplexes that are in each state, (ii) `{state}_{time}_scores.log`, which is a list of all scores of all structural models in that snapshot model, and (iii) `exp_comp{prot}.csv`, which is the experimental copy number for each protein (`{prot}`) as a function of time. Here, we copy files related to the snapshot models (`*.log` files) from the `modeling` directory, as we skipped computing snapshot models due to the computational expense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb887efe-0630-47b6-9bf1-85fa216a6816",
   "metadata": {
    "id": "bb887efe-0630-47b6-9bf1-85fa216a6816"
   },
   "outputs": [],
   "source": [
    "def merge_scores(fileA, fileB, outputFile):\n",
    "    \"\"\"\n",
    "    For each function merges scoresA.txt and scoresB.txt into {state}_{time}_scores.log\n",
    "\n",
    "    :param fileA: path to scoresA.txt\n",
    "    :param fileB: path to scoresB.txt\n",
    "    :param outputFile: path to output merged .log file named {state}_{time}_scores.log for each snapshot.\n",
    "    This type of .log file is used in crete_DAG to generate trajectory model.\n",
    "    \"\"\"\n",
    "    # open both files, so data can be extracted\n",
    "    with open(fileA, 'r') as file_a:\n",
    "        data_a = file_a.readlines()\n",
    "\n",
    "    with open(fileB, 'r') as file_b:\n",
    "        data_b = file_b.readlines()\n",
    "\n",
    "    # Merge the content of both files\n",
    "    merged_data = data_a + data_b\n",
    "\n",
    "    # Write the merged content into the output file\n",
    "    with open(outputFile, 'w') as output:\n",
    "        output.writelines(merged_data)\n",
    "\n",
    "def create_data_and_copy_files(state_dict, custom_source_dir1 = None, custom_source_dir2 = None, custom_source_dir3 = None):\n",
    "    \"\"\"\n",
    "    Copies three types of files important to generate trajectory models:\n",
    "    -.config files created with start_sim.py in Snapshot_Modeling (source_dir1)\n",
    "    -time-dependent stoichiometry data for each timepoint. Data should be presented in .csv file. With this function all\n",
    "    csv file in source_dir2 will be copied. These .csv files will be used in the exp_comp dictionary in create_DAG\n",
    "    function\n",
    "    -scoresA and scoresB for each snapshot created with imp sampcon exhaust\n",
    "    (source_dir1 + snapshot + good_scoring_models) are merged into total score .txt using merge_scores helper function.\n",
    "    All copied files are gathered in newly created './data/' directory, where everything is prepared for create_DAG\n",
    "    function.\n",
    "\n",
    "\n",
    "    :param state_dict (dict): state_dict: dictionary that defines the spatiotemporal model.\n",
    "           The keys are strings that correspond to each time point in the\n",
    "           stepwise temporal process. Keys should be ordered according to the\n",
    "           steps in the spatiotemporal process. The values are integers that\n",
    "           correspond to the number of possible states at that timepoint.\n",
    "    :param custom_source_dir1 (optional - str): Custom path to heterogeneity modeling dir (heterogeneity_modeling.py),\n",
    "    to copy .config files\n",
    "    :param custom_source_dir2 (optional - str): Custom path to stoichiometry data dir\n",
    "    :param custom_source_dir3 (optional - str): Custom path to snapshot modeling dir (start_sim.py), to copy .config\n",
    "    files and to access scoresA/scoresB (custom_source_dir3 + snapshot{state}_{time} + 'good_scoring_models')\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the destination directory if it does not exist (./data/). Here all the\n",
    "    destination_dir = './data/'\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "    # Path to heterogeneity modeling dir\n",
    "    if custom_source_dir1:\n",
    "        source_dir1 = custom_source_dir1\n",
    "    else:\n",
    "        source_dir1 = '../../Heterogeneity/Heterogeneity_Modeling/'\n",
    "\n",
    "    # Path to stoichiometry data dir\n",
    "    if custom_source_dir2:\n",
    "        source_dir2 = custom_source_dir2\n",
    "    else:\n",
    "        source_dir2 = '../../Input_Information/gen_FCS/'\n",
    "\n",
    "    # Path to snapshot modeling dir\n",
    "    if custom_source_dir3:\n",
    "        source_dir3 = custom_source_dir3\n",
    "    else:\n",
    "        source_dir3 = '../../Snapshots/Snapshots_Modeling/'\n",
    "\n",
    "    # Copy all .config files from the first source directory to the destination directory\n",
    "    try:\n",
    "        for file_name in os.listdir(source_dir1):\n",
    "            if file_name.endswith('.config'):\n",
    "                full_file_name = os.path.join(source_dir1, file_name)\n",
    "                if os.path.isfile(full_file_name):\n",
    "                    shutil.copy(full_file_name, destination_dir)\n",
    "        print(\".config files are copied\")\n",
    "    except Exception as e:\n",
    "        print(f\".config files cannot be copied. Try do do it manually. Reason for Error: {e}\")\n",
    "\n",
    "    # Copy all .csv stoichiometry files from the second source directory to the destination directory\n",
    "    try:\n",
    "        for file_name in os.listdir(source_dir2):\n",
    "            if file_name.endswith('.csv'):\n",
    "                full_file_name = os.path.join(source_dir2, file_name)\n",
    "                if os.path.isfile(full_file_name):\n",
    "                    shutil.copy(full_file_name, destination_dir)\n",
    "        print(\".csv stoichiometry files are copied\")\n",
    "    except Exception as e:\n",
    "        print(f\".csv stoichiometry files cannot be copied. Try do do it manually. Reason for Error: {e}\")\n",
    "\n",
    "    # Copy scoresA and scoresB from the snapshot_{state}_{time} directories and first source directory path\n",
    "    for time in state_dict.keys():\n",
    "        for state in range(1, state_dict[time] + 1):\n",
    "            dir_name = f\"snapshot{state}_{time}\"\n",
    "            good_scoring_path = \"good_scoring_models\"\n",
    "            file_a = os.path.join(source_dir3, dir_name, good_scoring_path, \"scoresA.txt\")\n",
    "            file_b = os.path.join(source_dir3, dir_name, good_scoring_path, \"scoresB.txt\")\n",
    "            output_file = os.path.join(destination_dir, f\"{state}_{time}_scores.log\") # name of the output file\n",
    "\n",
    "            try:\n",
    "                # Ensure the directory exists before try to read/write files\n",
    "                if os.path.exists(file_a) and os.path.exists(file_b):\n",
    "                    merge_scores(file_a, file_b, output_file) # call helper function to merge files\n",
    "                    print(f\"Scores for snapshot{state}_{time} have been merged and saved\")\n",
    "                else:  # many things can go wrong here, so it is good to know where is the problem\n",
    "                    print(f\"Path doesn't exist: {source_dir3}\")\n",
    "                    print(f\"Files not found in directory: {dir_name}\")\n",
    "                    print(f\"Files not found in directory: {file_a}\")\n",
    "                    print(f\"Files not found in directory: {file_b}\")\n",
    "                    print(f\"Output directory doesn't exist: {destination_dir}\")\n",
    "            except Exception as e:\n",
    "                print(f\"total scores files cannot be copied of merged. Reason for Error: {e}\")\n",
    "\n",
    "# copy all the relevant files for create_DAG\n",
    "# it is important that everything starts from main dir\n",
    "os.chdir(main_dir)\n",
    "state_dict = {'0min': 3, '1min': 3, '2min': 1}\n",
    "create_data_and_copy_files(state_dict, custom_source_dir1=main_dir, custom_source_dir2='../modeling/Input_Information/gen_FCS/', custom_source_dir3='../modeling/Snapshots/Snapshots_Modeling/')\n",
    "\n",
    "# then trajectory model is created based on the all copied data\n",
    "expected_subcomplexes = ['A', 'B', 'C']\n",
    "exp_comp = {'A': 'exp_compA.csv', 'B': 'exp_compB.csv', 'C': 'exp_compC.csv'}\n",
    "input = './data/'\n",
    "output = \"../output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4053cf-f2a3-4836-89f3-a8b2098109f2",
   "metadata": {
    "id": "7a4053cf-f2a3-4836-89f3-a8b2098109f2"
   },
   "source": [
    "Next, we compute the spatiotemporal model. The inputs we included are:\n",
    "- state_dict (dict): a dictionary that defines the spatiotemporal model. Keys are strings for each time point in the spatiotemporal process and values are integers corresponding to the number of snapshot models computed at that time point.\n",
    "- out_pdf (bool): whether to write the probability distribution function (pdf).\n",
    "- npaths (int): Number of states two write to a file (path*.txt).\n",
    "- input_dir (str): directory with the input information.\n",
    "- scorestr (str): final characters at the end of the score files.\n",
    "- output_dir (str): directory to which model will be written. Will be created if it does not exist.\n",
    "- spatio_temporal_rule (bool): whether to include our transition scoring term, which enforces that all proteins in the first snapshot model are included in the second snapshot model.\n",
    "- expected_subcomplexes (list): list of string objects, which is the subcomplexes to look when enforcing the spatiotemporal rule. Strings should be substrings of those in `{state}_{time}.config` files.\n",
    "- score_comp (bool): whether to score the composition of each snapshot model.\n",
    "- exp_comp_map (dictionary): key is a string with the name of each protein that will undergo composition scoring, value is the `.csv` file with the copy number data for that protein.\n",
    "- draw_dag (bool): whether to write out an image of the directed acyclic graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819bb205-fa52-42f9-a4ab-d2f7c3cff0ad",
   "metadata": {
    "id": "819bb205-fa52-42f9-a4ab-d2f7c3cff0ad"
   },
   "outputs": [],
   "source": [
    "nodes, graph, graph_prob, graph_scores = spatiotemporal.create_DAG(state_dict, out_pdf=True, npaths=3,\n",
    "                                                                       input_dir=input, scorestr='_scores.log',\n",
    "                                                                       output_dir=output, spatio_temporal_rule=True,\n",
    "                                                                       expected_subcomplexes=expected_subcomplexes,\n",
    "                                                                       score_comp=True, exp_comp_map=exp_comp,\n",
    "                                                                       draw_dag=True)\n",
    "os.chdir(main_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184fcec-05b9-4348-a9b8-d3673db00fe5",
   "metadata": {
    "id": "7184fcec-05b9-4348-a9b8-d3673db00fe5"
   },
   "source": [
    "After running `spatiotemporal.create_DAG`, a variety of outputs are written:\n",
    "- `cdf.txt`: the cumulative distribution function for the set of trajectories.\n",
    "- `pdf.txt`: the probability distribution function for the set of trajectories.\n",
    "- `labeled_pdf.txt`: Each row has 2 columns and represents a different trajectory. The first column labels a single trajectory as a series of snapshot models, where each snapshot model is written as `{state}_{time}|` in sequential order. The second column is the probability distribution function corresponding to that trajectory.\n",
    "- `dag_heatmap.eps` and `dag_heatmap`: image of the directed acyclic graph from the set of models.\n",
    "- `path*.txt`: files where each row includes a `{state}_{time}` string, so that rows correspond to the states visited over that trajectory. Files are numbered from the most likely path to the least likely path.\n",
    "\n",
    "Here, we take a look at one of these outputs, `labeled_pdf.txt`. From this file, we can see that the the first trajectory, which passes through snapshot 1 at time 0 min, snapshot 1 at time 1 min, and snapshot 1 at 2 min, is the highest weighted, and that the other trajectories have a much lower weight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ce237-5fd1-4d3e-8e84-5e29d2fbad94",
   "metadata": {
    "id": "148ce237-5fd1-4d3e-8e84-5e29d2fbad94"
   },
   "outputs": [],
   "source": [
    "with open('output/labeled_pdf.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3531c3de-3f5b-43f1-bd8c-5783d10ba2db",
   "metadata": {
    "id": "3531c3de-3f5b-43f1-bd8c-5783d10ba2db"
   },
   "source": [
    "If `graphviz` is availabe, the trajectory model will also be written as a directed acyclic graph. Each row in the graph is a single time point, and each node is shaded according to its weight in the trajectory model (purple). Here, we can view that directed aclyclic graph to show that the highest weighted trajectory passes through snapshot 1 at time 0 min, snapshot 1 at time 1 min, and snapshot 1 at 2 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b27a7d-2a3a-4f6d-a11e-0845f76d8d72",
   "metadata": {
    "id": "e7b27a7d-2a3a-4f6d-a11e-0845f76d8d72"
   },
   "outputs": [],
   "source": [
    "if os.path.exists('output/dag_heatmap.eps'):\n",
    "    # Convert EPS to PNG\n",
    "    img = Image.open('output/dag_heatmap.eps')\n",
    "    img.save('output/dag_heatmap.png')\n",
    "    # Display the PNG\n",
    "    display(Image.open('output/dag_heatmap.png'))\n",
    "else:\n",
    "    img = Image.open('images/Spatiotemporal_Model.png')\n",
    "    base_width=300\n",
    "    wpercent = (base_width / float(img.size[0]))\n",
    "    hsize = int((float(img.size[1]) * float(wpercent)))\n",
    "    img = img.resize((base_width, hsize))\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c016e-e5ba-4e3b-a76e-499750abb782",
   "metadata": {
    "id": "fb4c016e-e5ba-4e3b-a76e-499750abb782"
   },
   "source": [
    "\n",
    "# Trajectory modeling step 3: assessment<a id=\"notebook_trajectory_assess\"></a>\n",
    "\n",
    "Now that the set of spatiotemporal models has been constructed, we must evaluate these models. We can evaluate these models in at least 4 ways: estimating the sampling precision, comparing the model to data used to construct it, validating the model against data not used to construct it, and quantifying the precision of the model.\n",
    "\n",
    "## Sampling precision<a id=\"notebook_trajectory_sampling_precision\"></a>\n",
    "\n",
    "To begin, we calculate the sampling precision of the models. The sampling precision is calculated by using `spatiotemporal.create_DAG` to reconstruct the set of trajectories using 2 independent sets of samplings for snapshot models. Then, the overlap between these snapshot models is evaluated using `analysis.temporal_precision`, which takes in two `labeled_pdf` files.\n",
    "\n",
    "The temporal precision can take values between 1.0 and 0.0, and indicates the overlap between the two models in trajectory space. Hence, values close to 1.0 indicate a high sampling precision, while values close to 0.0 indicate a low sampling precision. Here, the value close to 1.0 indicates that sampling does not affect the weights of the trajectories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1436e33-81b6-400c-bf9c-7b667455265a",
   "metadata": {
    "id": "f1436e33-81b6-400c-bf9c-7b667455265a"
   },
   "outputs": [],
   "source": [
    "## 1 - calculation of temporal precision\n",
    "\n",
    "# 1 - copy_files_for_data (copy all relevant files into 'data' directory)\n",
    "def copy_files_for_data(state_dict, custom_source_dir1 = None, custom_source_dir2 = None, custom_source_dir3 = None):\n",
    "    \"\"\"\n",
    "    Copies three types of files important to generate trajectory models:\n",
    "    -.config files created with start_sim.py in Snapshot_Modeling (source_dir1)\n",
    "    -time-dependent stoichiometry data for each timepoint. Data should be presented in .csv file. With this function all\n",
    "    csv file in source_dir2 will be copied. These .csv files will be used in the exp_comp dictionary in create_DAG\n",
    "    function\n",
    "    -scoresA and scoresB for each snapshot created with imp sampcon exhaust\n",
    "    (source_dir1 + snapshot + good_scoring_models) are merged into total score .txt using merge_scores helper function.\n",
    "    All copied files are gathered in newly created './data/' directory, where everything is prepared for create_DAG\n",
    "    function.\n",
    "\n",
    "\n",
    "    :param state_dict (dict): state_dict: dictionary that defines the spatiotemporal model.\n",
    "           The keys are strings that correspond to each time point in the\n",
    "           stepwise temporal process. Keys should be ordered according to the\n",
    "           steps in the spatiotemporal process. The values are integers that\n",
    "           correspond to the number of possible states at that timepoint.\n",
    "    :param custom_source_dir1 (optional - str): Custom path to heterogeneity modeling dir (heterogeneity_modeling.py),\n",
    "    to copy .config files\n",
    "    :param custom_source_dir2 (optional - str): Custom path to stoichiometry data dir\n",
    "    :param custom_source_dir3 (optional - str): Custom path to snapshot modeling dir (start_sim.py), to copy .config\n",
    "    files and to access scoresA/scoresB (custom_source_dir3 + snapshot{state}_{time} + 'good_scoring_models')\n",
    "    \"\"\"\n",
    "    # Create the destination directory for all the data copied in this function\n",
    "    destination_dir = './data/'\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "    # path to snapshot modeling dir\n",
    "    if custom_source_dir1:\n",
    "        source_dir1 = custom_source_dir1\n",
    "    else:\n",
    "        source_dir1 = '../../Heterogeneity/Heterogeneity_Modeling/'\n",
    "\n",
    "    # path to stoichiometry data dir\n",
    "    if custom_source_dir2:\n",
    "        source_dir2 = custom_source_dir1\n",
    "    else:\n",
    "        source_dir2 = '../../Input_Information/gen_FCS/'\n",
    "\n",
    "    # path to snapshot modeling dir\n",
    "    if custom_source_dir3:\n",
    "        source_dir3 = custom_source_dir3\n",
    "    else:\n",
    "        source_dir3 = '../../Snapshots/Snapshots_Modeling/'\n",
    "\n",
    "    # Copy all .config files from the first source directory to the destination directory\n",
    "    try:\n",
    "        for file_name in os.listdir(source_dir1):\n",
    "            if file_name.endswith('.config'):\n",
    "                full_file_name = os.path.join(source_dir1, file_name)\n",
    "                if os.path.isfile(full_file_name):\n",
    "                    shutil.copy(full_file_name, destination_dir)\n",
    "        print(\".config files are copied\")\n",
    "    except Exception as e:\n",
    "        print(f\".config files cannot be copied. Try do do it manually. Reason for Error: {e}\")\n",
    "\n",
    "    # Copy all .csv stoichiometry files from the second source directory to the destination directory\n",
    "    try:\n",
    "        for file_name in os.listdir(source_dir2):\n",
    "            if file_name.endswith('.csv'):\n",
    "                full_file_name = os.path.join(source_dir2, file_name)\n",
    "                if os.path.isfile(full_file_name):\n",
    "                    shutil.copy(full_file_name, destination_dir)\n",
    "        print(\".csv stoichiometry files are copied\")\n",
    "    except Exception as e:\n",
    "        print(f\".csv stoichiometry files cannot be copied. Try do do it manually. Reason for Error: {e}\")\n",
    "\n",
    "    # Copy scoresA and scoresB from the snapshot_{state}_{time} directories and first source directory path\n",
    "    try:\n",
    "        for time in state_dict.keys():\n",
    "            for state in range(1, state_dict[time] + 1):\n",
    "                snapshot_dir = os.path.join(source_dir3, f'snapshot{state}_{time}')\n",
    "                good_scoring_models_dir = os.path.join(snapshot_dir, 'good_scoring_models')\n",
    "                if os.path.isdir(good_scoring_models_dir):\n",
    "                    for score_file in ['scoresA.txt', 'scoresB.txt']:\n",
    "                        full_file_name = os.path.join(good_scoring_models_dir, score_file)\n",
    "                        if os.path.isfile(full_file_name):\n",
    "                            new_file_name = f'{state}_{time}_{os.path.splitext(score_file)[0]}.log'\n",
    "                            shutil.copy(full_file_name, os.path.join(destination_dir, new_file_name))\n",
    "                            print(f\"Copied {full_file_name} to {os.path.join(destination_dir, new_file_name)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"scoresA.txt and scoresB.txt cannot be copied. Try do do it manually. Reason for Error: {e}\")\n",
    "\n",
    "os.chdir(main_dir)\n",
    "# copy all the relevant files\n",
    "copy_files_for_data(state_dict, custom_source_dir1='../modeling/Heterogeneity/Heterogeneity_Modeling/',\n",
    "                   custom_source_dir2='../modeling/Input_Information/gen_FCS/',\n",
    "                   custom_source_dir3='../modeling/Snapshots/Snapshots_Modeling/')\n",
    "\n",
    "# create two independent DAGs\n",
    "expected_subcomplexes = ['A', 'B', 'C']\n",
    "exp_comp = {'A': 'exp_compA.csv', 'B': 'exp_compB.csv', 'C': 'exp_compC.csv'}\n",
    "input = \"./data/\"\n",
    "outputA = \"../output_modelA/\"\n",
    "outputB = \"../output_modelB/\"\n",
    "\n",
    "# Output from sampling precision and model precision to be saved in united dir: analysis_output_precision\n",
    "analysis_output = \"./analysis_output_precision/\"\n",
    "os.makedirs(analysis_output, exist_ok=True)\n",
    "\n",
    "nodesA, graphA, graph_probA, graph_scoresA = spatiotemporal.create_DAG(state_dict, out_pdf=True, npaths=3,\n",
    "                                                                        input_dir=input, scorestr='_scoresA.log',\n",
    "                                                                        output_dir=outputA,\n",
    "                                                                        spatio_temporal_rule=True,\n",
    "                                                                        expected_subcomplexes=expected_subcomplexes,\n",
    "                                                                        score_comp=True, exp_comp_map=exp_comp,\n",
    "                                                                        draw_dag=False)\n",
    "\n",
    "os.chdir(main_dir)\n",
    "nodesB, graphB, graph_probB, graph_scoresB = spatiotemporal.create_DAG(state_dict, out_pdf=True, npaths=3,\n",
    "                                                                        input_dir=input, scorestr='_scoresB.log',\n",
    "                                                                        output_dir=outputB,\n",
    "                                                                        spatio_temporal_rule=True,\n",
    "                                                                        expected_subcomplexes=expected_subcomplexes,\n",
    "                                                                        score_comp=True, exp_comp_map=exp_comp,\n",
    "                                                                        draw_dag=False)\n",
    "\n",
    "## 1 - analysis\n",
    "analysis.temporal_precision(outputA + 'labeled_pdf.txt', outputB + 'labeled_pdf.txt',\n",
    "                            output_fn='.' + analysis_output + 'temporal_precision.txt')\n",
    "os.chdir(main_dir)  # it is crucial that after each step, directory is changed back to main\n",
    "print(\"Step 1: calculation of temporal precision IS COMPLETED\")\n",
    "print(\"\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce48a52-b06c-4e09-80a5-debe0fd2cba2",
   "metadata": {
    "id": "6ce48a52-b06c-4e09-80a5-debe0fd2cba2"
   },
   "source": [
    "## Model precision<a id=\"notebook_trajectory_precision\"></a>\n",
    "\n",
    "Next, we calculate the precision of the model, using `analysis.precision`. Here, the model precision calculates the number of trajectories with high weights. The precision ranges from 1.0 to 1/d, where d is the number of trajectories. Values approaching 1.0 indicate the model set can be described by a single trajectory, while values close to 1/d indicate that all trajectories have similar weights.\n",
    "\n",
    "The `analysis.precision` function reads in the `labeled_pdf` of the complete model, and calculates the precision of the model. The value close to 1.0 indicates that the set of models can be sufficiently represented by a single trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3669407-4ddc-4e1f-b8ba-c8638024cd56",
   "metadata": {
    "id": "d3669407-4ddc-4e1f-b8ba-c8638024cd56"
   },
   "outputs": [],
   "source": [
    "## 2 - calculation of precision of the model\n",
    "\n",
    "# precision is calculated from .labeled_pdf.txt in Trajectories_Modeling dir\n",
    "trajectories_modeling_input_dir = \"./output/\"\n",
    "\n",
    "analysis.precision(trajectories_modeling_input_dir + 'labeled_pdf.txt', output_fn=analysis_output + 'precision.txt')\n",
    "\n",
    "os.chdir(main_dir)  # it is crucial that after each step, directory is changed back to main\n",
    "print(\"Step 2: calculation of precision of the model IS COMPLETED\")\n",
    "print(\"\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44c8b5-93d8-4ab8-b8cc-d6a70168c0eb",
   "metadata": {
    "id": "8a44c8b5-93d8-4ab8-b8cc-d6a70168c0eb"
   },
   "source": [
    "## Comparison against data used in model construction<a id=\"notebook_trajectory_comparison\"></a>\n",
    "\n",
    "We then evaluate the model against data used in model construction. First, we can calculate the cross-correlation between the original EM map and the forward density projected from each snapshot model. This calculation is too computationally expensive for this notebook, but can be found in `modeling/Trajectories/Trajectories_Assessment`, where we wrote the `ccEM` function to perform this comparison for all snapshot models.\n",
    "\n",
    "```python\n",
    "# 3a - comparison of the model to data used in modeling (EM)\n",
    "exp_mrc_base_path = \"../../Input_Information/ET_data/add_noise\"\n",
    "ccEM(exp_mrc_base_path)\n",
    "print(\"Step 3a: ET validation IS COMPLETED\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "```\n",
    "\n",
    "The output of this analysis is saved to `ccEM_calculations.txt` in the `ccEM_output`, shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc40db3-5422-4257-aab3-b216fe2af388",
   "metadata": {
    "id": "1bc40db3-5422-4257-aab3-b216fe2af388"
   },
   "outputs": [],
   "source": [
    "os.chdir(main_dir)\n",
    "with open('../modeling/Trajectories/Trajectories_Assessment/ccEM_output/ccEM_calculations.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a2cc71-be2b-40e8-aa62-3cceaa60e30f",
   "metadata": {
    "id": "83a2cc71-be2b-40e8-aa62-3cceaa60e30f"
   },
   "source": [
    "We are particularly interested in how the good scoring trajectory compares to the experimental data. Here, we plot the cross-correlation between the forward density of snapshot models in the highest weighted trajectory and the EM map (note: these values are directly taken from `ccEM_calculations.txt`).\n",
    "\n",
    "<img src=\"https://github.com/salilab/imp_spatiotemporal_tutorial/blob/main/Jupyter/images/EM_CC.png?raw=1\" width=\"300px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6e0a0-98d5-4b63-a839-221a4fdc493b",
   "metadata": {
    "id": "70a6e0a0-98d5-4b63-a839-221a4fdc493b"
   },
   "source": [
    "After comparing the model to EM data, we aimed to compare the model to copy number data, and wrote the `forward_model_copy_number` function to evaluate the copy numbers from our set of trajectories. The output of `forward_model_copy_number` is written in `forward_model_copy_number/`. The folder contains `CN_prot_{prot}.txt` files for each protein, which have the mean and standard deviation of protein copy number at each time point. We can then plot these copy numbers from the forward models against those from the experiment, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f7c451-2b04-4e8b-b39c-16c493e8a1a6",
   "metadata": {
    "id": "f3f7c451-2b04-4e8b-b39c-16c493e8a1a6"
   },
   "outputs": [],
   "source": [
    "def read_labeled_pdf(pdf_file):\n",
    "    \"\"\"\n",
    "    Function to read in a labeled probability distribution file output by spatiotemporal.create_DAG.\n",
    "    Used to determine protein copy numbers by forward_model_copy_number.\n",
    "    :param pdf_file (str): sting for the path of the labeled probability distribution file output by\n",
    "    spatiotemporal.create_DAG.\n",
    "    :return prob_dict (dict): dictionary defining the spatiotemporal model. Each key is a state, and each value is the\n",
    "    probability of that state.\n",
    "    \"\"\"\n",
    "    # create blank dictonary to store the results\n",
    "    prob_dict = {}\n",
    "    # read in labeled pdf file\n",
    "    old = open(pdf_file, 'r')\n",
    "    line = old.readline()\n",
    "    # store the path through various nodes, as well as the probability of that path\n",
    "    while line:\n",
    "        line_split = line.split()\n",
    "        # assumes the first string is the trajectory string, the second string is the probability\n",
    "        if len(line_split) > 1:\n",
    "            # use # for comments\n",
    "            if line_split[0]=='#':\n",
    "                pass\n",
    "            else:\n",
    "                trj = line_split[0]\n",
    "                prob = float(line_split[1])\n",
    "                # store in dictionary\n",
    "                prob_dict[trj] = prob\n",
    "        line = old.readline()\n",
    "    old.close()\n",
    "    return prob_dict\n",
    "\n",
    "def copy_number_from_state(prot_list,trj,custom_data_folder = None):\n",
    "    \"\"\"\n",
    "    For a trajectory, returns an array of protein copy numbers as a function of time. Used by\n",
    "    forward_model_copy_number().\n",
    "    :param prot_list (list): list of proteins in the model. These proteins are searched for in each config file.\n",
    "    :param trj (str): string defining a single trajectory.\n",
    "    :param custom_data_folder (str, optional): path to custom data folder. Defaults to None, which points to '../data/'\n",
    "    :return _prots (array): 2D array of protein copy numbers. The first index loops over the time,\n",
    "    while the second index value loops over the protein (ordered as A, B, C).\n",
    "    :return N (int): Number of time points in each trajectory.\n",
    "    \"\"\"\n",
    "    # find folder with config files\n",
    "    if custom_data_folder:\n",
    "        data_folder = custom_data_folder\n",
    "    else:\n",
    "        data_folder = 'data/'\n",
    "\n",
    "    # split the trajectory into a list of individual states\n",
    "    state_list=trj.split('|')\n",
    "    state_list=state_list[:-1]\n",
    "\n",
    "    N = len(state_list)\n",
    "    # Map from index to protein: 0 - A, 1- B, 2- C\n",
    "    _prots = np.zeros((N, len(prot_list)))\n",
    "\n",
    "    # Grab _prots from .config file\n",
    "    for i in range(0, N):\n",
    "        prot_file = data_folder + state_list[i] + '.config'\n",
    "        to_read = open(prot_file, 'r')\n",
    "        line = to_read.readline()\n",
    "        while line:\n",
    "            # for each line, check if the protein is in that line\n",
    "            for prot_index in range(len(prot_list)):\n",
    "                if prot_list[prot_index] in line:\n",
    "                    _prots[i, prot_index] += 1\n",
    "            line = to_read.readline()\n",
    "\n",
    "    return _prots,N\n",
    "\n",
    "def forward_model_copy_number(prot_list,custom_labeled_pdf=None):\n",
    "    \"\"\"\n",
    "    Code to perform copy number analysis on each protein in the model. Writes output files where each row is ordered\n",
    "    according to the time point in the model and the first column is the mean copy number, while the second column is\n",
    "    the standard deviation in copy number.\n",
    "    :param prot_list (list): list of proteins in the model. These proteins are searched for in each config file.\n",
    "    :param custom_labeled_pdf (str, optional): path to custom labeled probability distribution file output by\n",
    "    spatiotemporal.create_DAG.\n",
    "    \"\"\"\n",
    "    # find folder with config files\n",
    "    if custom_labeled_pdf:\n",
    "        _labeled_pdf = custom_labeled_pdf\n",
    "    else:\n",
    "        _labeled_pdf = '../Trajectories_Modeling/output/labeled_pdf.txt'\n",
    "\n",
    "    # Read in labeled_pdf file into a dictionary. Each trajectory is listed as a dictionary,\n",
    "    # with keys as the trajectory and the values as the probability of that trajectory\n",
    "    prob_dict = read_labeled_pdf(_labeled_pdf)\n",
    "\n",
    "    # Loop over the full dictionary. Create a list with 2 values:\n",
    "    # 1) the probability of the state, 2) the protein copy number of that state.\n",
    "    key_list = prob_dict.keys()\n",
    "    prot_prob = []\n",
    "    for key in key_list:\n",
    "        CN,N_times = copy_number_from_state(prot_list,key)\n",
    "        prot_prob.append([prob_dict[key], CN])\n",
    "\n",
    "    # Construct the full path to the output directory\n",
    "    dir_name = \"forward_model_copy_number\"\n",
    "    full_path = os.path.join(main_dir, dir_name)\n",
    "    os.makedirs(full_path, exist_ok=True)\n",
    "    os.chdir(full_path)\n",
    "\n",
    "    # Determine copy number from the prot_prob\n",
    "    for index in range(len(prot_prob[0][1][0])):\n",
    "        copy_number = np.zeros((N_times, 2))\n",
    "        # calculate mean\n",
    "        for state in prot_prob:\n",
    "            for i in range(N_times):\n",
    "                copy_number[i, 0] += state[0] * state[1][i][index]\n",
    "        # calculate std deviation\n",
    "        for state in prot_prob:\n",
    "            for i in range(N_times):\n",
    "                # Calculate variance\n",
    "                copy_number[i, 1] += state[0] * ((state[1][i][index] - copy_number[i, 0]) ** 2)\n",
    "        # Take square root to get the standard deviation\n",
    "        copy_number[:, 1] = np.sqrt(copy_number[:, 1])\n",
    "        # save to file\n",
    "        np.savetxt('CN_prot_'+prot_list[index]+'.txt', copy_number, header='mean CN\\tstd CN')\n",
    "\n",
    "# 3b - comparison of the model to data used in modeling (copy number)\n",
    "os.chdir(main_dir)  # it is crucial that after each step, directory is changed back to main\n",
    "forward_model_copy_number(expected_subcomplexes,custom_labeled_pdf='output/labeled_pdf.txt')\n",
    "os.chdir(main_dir)\n",
    "# plot copy number against the experimental copy number\n",
    "# Import experimental data\n",
    "expA=np.loadtxt(\"../modeling/Input_Information/gen_FCS/A_v2.txt\")\n",
    "expB=np.loadtxt(\"../modeling/Input_Information/gen_FCS/B_v2.txt\")\n",
    "expC=np.loadtxt(\"../modeling/Input_Information/gen_FCS/C_v2.txt\")\n",
    "# Import results of forward_model_copy_number\n",
    "modelA=np.loadtxt(\"forward_model_copy_number/CN_prot_A.txt\")\n",
    "modelB=np.loadtxt(\"forward_model_copy_number/CN_prot_B.txt\")\n",
    "modelC=np.loadtxt(\"forward_model_copy_number/CN_prot_C.txt\")\n",
    "time=np.asarray([0,1,2]);\n",
    "# Plot data\n",
    "fig, ((ax1), (ax2), (ax3)) = plt.subplots(3, 1, figsize=(5,8))\n",
    "ax1.errorbar(expA[:,0], expA[:,1], yerr=expA[:,2], color=grey)\n",
    "ax1.errorbar(time, modelA[:,0], yerr=modelA[:,1], color=blue1)\n",
    "ax1.plot(time, modelA[:,0], 'o', color=blue1)\n",
    "ax1.set_ylabel('Copy number [A]')\n",
    "ax1.set_xlabel('Time [min]')\n",
    "ax1.set_xlim(-0.2,2.7)\n",
    "ax1.set_ylim(-1,2.5)\n",
    "ax1.legend(['model', 'experiment'], loc='upper left')\n",
    "ax2.errorbar(expB[:,0], expB[:,1], yerr=expB[:,2], color=grey)\n",
    "ax2.errorbar(time, modelB[:,0], yerr=modelB[:,1], color=blue1)\n",
    "ax2.plot(time, modelB[:,0], 'o', color=blue1)\n",
    "ax2.set_ylabel('Copy number [B]')\n",
    "ax2.set_xlabel('Time [min]')\n",
    "ax2.set_xlim(-0.2, 2.7)\n",
    "ax2.set_ylim(-0.5, 3)\n",
    "ax2.legend(['model', 'experiment'], loc='upper left')\n",
    "ax3.errorbar(expC[:,0], expC[:,1], yerr=expC[:,2], color=grey)\n",
    "ax3.errorbar(time, modelC[:,0], yerr=modelC[:,1], color=blue1)\n",
    "ax3.plot(time, modelC[:,0], 'o', color=blue1)\n",
    "ax3.set_ylabel('Copy number [C]')\n",
    "ax3.set_xlabel('Time [min]')\n",
    "ax3.set_xlim(-0.2, 2.7)\n",
    "ax2.set_ylim(-0.5, 3)\n",
    "ax3.legend(['model', 'experiment'], loc='upper left')\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.show()\n",
    "print(\"Step 3b: copy number validation IS COMPLETED\")\n",
    "print(\"\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b6e15-1147-4706-bb26-44125961f9fb",
   "metadata": {
    "id": "305b6e15-1147-4706-bb26-44125961f9fb"
   },
   "source": [
    "## Validation against data not used in model construction<a id=\"notebook_trajectory_validation\"></a>\n",
    "\n",
    "Finally, we aim to compare the model to data not used in model construction. Specifically, we reserved SAXS data for model validation. We aimed to compare the forward scattering profile from the centroid structural model of each snapshot model to the experimental profile. To make this comparison, we wrote functions that converted each centroid RMF to a PDB (`convert_rmfs`), copied the experimental SAXS profiles to the appropriate folder (`copy_SAXS_dat_files`), and ran [FoXS](https://integrativemodeling.org/tutorials/foxs/foxs.html) on each PDB to evaluate its agreement to the experimental profile (`process_foxs`).\n",
    "\n",
    "The output of this analysis is written to `SAXS_comparison`. Standard FoXS outputs are available for each snapshot model (`snapshot{state}_{time}.*`). In particular, the `.fit` files include the forward and experimental profiles side by side, with the $\\chi^2$ for the fit. Further, the `{time}_FoXS.*` files include the information for all snapshot models at that time point, including plots of each profile in comparison to the experimental profile (`{time}_FoXS.png`, which we show here). These plots include a $\\chi^2$, which quantitatively compares each snapshot model to the experimental profile. We note that the $\\chi^2$ are substantially lower for the models along the highest weighted trajectory (1_0min, 1_1min, and 1_2min) than for other models, indicating that the highest weighted trajectory is in better agreement with the experimental SAXS data than other possible trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc728a-6159-4317-94eb-525dccc780aa",
   "metadata": {
    "id": "d5cc728a-6159-4317-94eb-525dccc780aa"
   },
   "outputs": [],
   "source": [
    "# 4a - SAXS\n",
    "\"\"\"\n",
    "Comparing center models of the most dominant cluster for each snapshot (rmfs) to the SAXS data for each time point\n",
    " can be done in two steps:\n",
    "-converting rmfs to pdb files\n",
    "-comparing pdbs of each snapshot to experimental SAXS profile using FoXS\n",
    "\"\"\"\n",
    "\n",
    "def convert_rmfs(state_dict, model, custom_path=None):\n",
    "    \"\"\"\n",
    "    The purpose of this function is to automate the conversion of RMF files into PDB files for all the states from\n",
    "    state_dict. Created PDBs are further used in comparison of SAXS profiles using FoXS. Additionally, they can be\n",
    "    used for comparison to native PDB if available.\n",
    "\n",
    "    :param state_dict (dict): dictionary that defines the spatiotemporal model.\n",
    "           The keys are strings that correspond to each time point in the\n",
    "           stepwise temporal process. Keys should be ordered according to the\n",
    "           steps in the spatiotemporal process. The values are integers that\n",
    "           correspond to the number of possible states at that timepoint.\n",
    "    :param model (str): An IMP (Integrative Modeling Platform) model object.\n",
    "    :param custom_path (optional - str): A custom path for the RMF file, allowing for flexibility in file location\n",
    "    (should be compliant with stat_dict).\n",
    "    \"\"\"\n",
    "\n",
    "    for time in state_dict.keys():\n",
    "        for state in range(1, state_dict[time] + 1):\n",
    "            if custom_path:\n",
    "                sim_rmf = custom_path # option for custom path\n",
    "            else:\n",
    "                sim_rmf = f\"../../modeling/Snapshots/Snapshots_Assessment/exhaust_{state}_{time}/cluster.0/cluster_center_model.rmf3\"\n",
    "\n",
    "            pdb_output = f\"snapshot{state}_{time}.pdb\" # define the output of converted .pdb file\n",
    "\n",
    "            if os.path.exists(sim_rmf):\n",
    "                try:\n",
    "                    rmf_fh = RMF.open_rmf_file_read_only(sim_rmf) # open rmf file for reading\n",
    "                    rmf_hierarchy = IMP.rmf.create_hierarchies(rmf_fh, model)[0] # extract 1st hierarchy\n",
    "                    IMP.atom.write_pdb_of_c_alphas(rmf_hierarchy, pdb_output) # write coordinates of CA to .pdb\n",
    "                    print(f\"Finishing: snapshot{state}_{time}.pdb\")\n",
    "                except Exception as e:\n",
    "                    print(f\"{sim_rmf} is empty or there is another problem: {e}\")\n",
    "\n",
    "\n",
    "def copy_SAXS_dat_files(custom_src_dir = None):\n",
    "    \"\"\"\n",
    "    Copies all files ending with .dat from the specified directory to the current directory.\n",
    "\n",
    "    :param custom_src_dir (optional - str): Path to the source directory\n",
    "    \"\"\"\n",
    "    if custom_src_dir:\n",
    "        src_dir = custom_src_dir\n",
    "    else:\n",
    "        src_dir = '../../../Input_Information/gen_SAXS'\n",
    "    try:\n",
    "        files = os.listdir(src_dir) # Get the list of all files in the src_dir directory\n",
    "        dat_files = [f for f in files if f.endswith('.dat')] # Filter out files that end with .dat\n",
    "\n",
    "        # Copy each .dat file to the current directory, so FoXS can be used\n",
    "        for file_name in dat_files:\n",
    "            full_file_name = os.path.join(src_dir, file_name)\n",
    "            if os.path.isfile(full_file_name):\n",
    "                shutil.copy(full_file_name, os.getcwd())\n",
    "                # print(f\"Copied: {full_file_name} to {main_dir}\")\n",
    "\n",
    "        print(\"All .dat files have been copied successfully...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "def process_foxs(state_dict, custom_dat_file = None):\n",
    "    \"\"\"\n",
    "    This function automates the FoXS analysis for all specified time points in a single execution. It processes PDB\n",
    "    files generated by the convert_rmfs function and uses SAXS data copied with the copy_SAXS function. All of this\n",
    "    data should be present in the current running directory.\n",
    "    FoXS tutorial is available here: https://integrativemodeling.org/tutorials/foxs/foxs.html\n",
    "\n",
    "    :param state_dict (dict): dictionary that defines the spatiotemporal model.\n",
    "           The keys are strings that correspond to each time point in the\n",
    "           stepwise temporal process. Keys should be ordered according to the\n",
    "           steps in the spatiotemporal process. The values are integers that\n",
    "           correspond to the number of possible states at that timepoint.\n",
    "    :param custom_dat_file (optional - str)): A custom name of SAXS files for each time point (should be\n",
    "    compliant with stat_dict)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    print(\"...lets proceed to FoXS\")\n",
    "\n",
    "    for time in state_dict.keys():\n",
    "        try:\n",
    "            if state_dict[time] > 1:\n",
    "                # if there is more than one state in timepoint, FoXS creates fit.plt and it should be renamed\n",
    "                if custom_dat_file:\n",
    "                    dat_file = custom_dat_file\n",
    "                else:\n",
    "                    dat_file = f\"{time}_exp.dat\"\n",
    "\n",
    "                pdb_files = \" \".join([f\"snapshot{state}_{time}.pdb\" for state in range(1, state_dict[time] + 1)])\n",
    "\n",
    "                command1 = f\"foxs -r -g {pdb_files} {dat_file}\"\n",
    "                # example how FoXS command should look like: foxs -r -g snapshot1_0min.pdb snapshot2_0min.pdb snapshot3_0min.pdb 0min_exp.dat\n",
    "                os.system(command1)\n",
    "                print(f\"FoXS for {time} is calculated and ready to create a plot. Nr of states is: {state_dict[time]}\")\n",
    "\n",
    "                command2 = f\"gnuplot fit.plt\" # create plot from .plt code\n",
    "                os.system(command2)\n",
    "\n",
    "                command3 = f\"mv fit.plt {time}_FoXS.plt\" # rename .plt to avoid to be overwritten\n",
    "                os.system(command3)\n",
    "\n",
    "                command4 = f\"mv fit.png {time}_FoXS.png\" # rename plot to avoid to be overwritten\n",
    "                os.system(command4)\n",
    "\n",
    "                # Plot the data\n",
    "                img = Image.open(f'{time}_FoXS.png')\n",
    "                base_width=600\n",
    "                wpercent = (base_width / float(img.size[0]))\n",
    "                hsize = int((float(img.size[1]) * float(wpercent)))\n",
    "                img = img.resize((base_width, hsize))\n",
    "                display(img)\n",
    "\n",
    "            elif state_dict[time] == 1:\n",
    "                print(f\"There is only one state in {time}\")\n",
    "                dat_file1 = f\"{time}_exp.dat\"\n",
    "                pdb_file1 = f\"snapshot1_{time}.pdb\"\n",
    "\n",
    "                command5 = f\"foxs -r -g {pdb_file1} {dat_file1}\"\n",
    "                os.system(command5)\n",
    "                print(f\"FoXS for {time} is calculated and ready to create a plot. Nr of states is: {state_dict[time]}\")\n",
    "\n",
    "                command6 = f\"gnuplot snapshot1_{time}_{time}_exp.plt\"\n",
    "                os.system(command6)\n",
    "\n",
    "                command7 = f\"mv snapshot1_{time}_{time}_exp.plt {time}_FoXS.plt\"\n",
    "                os.system(command7)\n",
    "\n",
    "                command8 = f\"mv snapshot1_{time}_{time}_exp.png {time}_FoXS.png\"\n",
    "                os.system(command8)\n",
    "\n",
    "                # Plot the data\n",
    "                img = Image.open(f'{time}_FoXS.png')\n",
    "                base_width=600\n",
    "                wpercent = (base_width / float(img.size[0]))\n",
    "                hsize = int((float(img.size[1]) * float(wpercent)))\n",
    "                img = img.resize((base_width, hsize))\n",
    "                display(img)\n",
    "            else:\n",
    "                print(f\"There is no states in this timepoint. Check stat_dict.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"FoXS can not be executed properly due to following problem: {e}\")\n",
    "\n",
    "# 4a - SAXS\n",
    "os.chdir(main_dir)  # it is crucial that after each step, directory is changed back to main\n",
    "SAXS_output = \"./SAXS_comparison/\"\n",
    "os.makedirs(SAXS_output, exist_ok=True)\n",
    "os.chdir(SAXS_output)\n",
    "model = IMP.Model()\n",
    "convert_rmfs(state_dict, model)\n",
    "copy_SAXS_dat_files(custom_src_dir='../../modeling/Input_Information/gen_SAXS')\n",
    "process_foxs(state_dict)\n",
    "os.chdir(main_dir)\n",
    "print(\"Step 4a: SAXS validation IS COMPLETED\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "os.chdir(main_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ebc14c-1eba-45ff-b98e-2938e65057a4",
   "metadata": {
    "id": "d4ebc14c-1eba-45ff-b98e-2938e65057a4"
   },
   "source": [
    "As our model was generated from synthetic data, the ground truth structure is known at each time point. In addition to validating the model by assessing its comparison to SAXS data, we could approximate the model accuracy by comparing the snapshot model to the PDB structure, although this comparison is not perfect as the PDB structure was used to inform the structure of *rigid bodies* in the snapshot model. To do so, we wrote a function (`RMSD`) that calculates the RMSD between each structural model and the orignal PDB. The function is too computationally expensive to run in this notebook, but is found in the `Trajectories/Trajectories_Assessment/` folder and is demonstrated below.\n",
    "\n",
    "```python\n",
    "# 4b - RMSD\n",
    "os.chdir(main_dir)  # it is crucial that after each step, directory is changed back to main\n",
    "pdb_path = \"../../Input_Information/PDB/3rpg.pdb\"\n",
    "RMSD(pdb_path=pdb_path, custom_n_plot=20)\n",
    "print(\"Step 4b: RMSD validation IS COMPLETED\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "```\n",
    "\n",
    "The output of this function is written in `RMSD_calculation_output`. The function outputs `rmsd_{state}_{time}.png` files, which plots the RMSD for each structural model within each snapshot model. This data is then summarized in `RMSD_analysis.txt`, which includes the minimum RMSD, average RMSD, and number of structural models in each snapshot model (shown below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92c3b49-98e4-401f-9fe7-e79b9a552247",
   "metadata": {
    "id": "a92c3b49-98e4-401f-9fe7-e79b9a552247"
   },
   "outputs": [],
   "source": [
    "os.chdir(main_dir)  # it is crucial that after each step, directory is changed back to main\n",
    "with open('../modeling/Trajectories/Trajectories_Assessment/RMSD_calculation_output/RMSD_analysis.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e1a2f6-7258-462d-bde3-7734743b5aa1",
   "metadata": {
    "id": "64e1a2f6-7258-462d-bde3-7734743b5aa1"
   },
   "source": [
    "Next, we can evaluate the accuracy of the model by comparing this average RMSD to the assembled structure (light red) to the sampling precision of each snapshot model (dark red). We note that this is generally not possible, because in most biological applications the ground truth is not known. In this case, if the average RMSD to the PDB structure is smaller than the sampling precision, the PDB structure lies within the precision of the model. We find that the RMSD is within 1.5 Å of the sampling precision at all time points, indicating that the model lies within 1.5 Å of the ground truth.\n",
    "\n",
    "<img src=\"https://github.com/salilab/imp_spatiotemporal_tutorial/blob/main/Jupyter/images/RMSD.png?raw=1\" width=\"300px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f7a12c-d7e9-49ed-9a16-b713c3b4762a",
   "metadata": {
    "id": "98f7a12c-d7e9-49ed-9a16-b713c3b4762a"
   },
   "source": [
    "# Next steps<a id=\"notebook_Conclusion\"></a>\n",
    "\n",
    "After assessing our model, we can must decide if the model is sufficient to answer biological questions of interest. If the model does not have sufficient precision for the desired application, assessment of the current model can be used to inform which new experiments may help improve the next iteration of the model. The [integrative spatiotemporal modeling procedure](https://integrativemodeling.org/tutorials/spatiotemporal/index.html#steps) can then be repeated iteratively, analogous to [integrative modeling of static structures](https://integrativemodeling.org/2.22.0/doc/manual/intro.html#procedure).\n",
    "\n",
    "If the model is sufficient to provide insight into the biological process of interest, the user may decide that it is ready for publication. In this case, the user should create an [mmCIF file](https://mmcif.wwpdb.org/) to deposit the model in the [PDB-IHM database](https://pdb-ihm.org/). This procedure is explained in the [deposition tutorial](https://integrativemodeling.org/tutorials/deposition/develop/).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
